{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6436ed30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/29 08:49:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/11/29 08:49:58 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-11-29 08:49:59, Running 1 Workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from cerebro.backend import SparkBackend\n",
    "from cerebro.keras import SparkEstimator\n",
    "\n",
    "# datas storage for intermediate data and model artifacts.\n",
    "from cerebro.storage import LocalStore, HDFSStore\n",
    "\n",
    "# Model selection/AutoML methods.\n",
    "from cerebro.tune import GridSearch, RandomSearch, TPESearch\n",
    "\n",
    "# Utility functions for specifying the search space.\n",
    "from cerebro.tune import hp_choice, hp_uniform, hp_quniform, hp_loguniform, hp_qloguniform\n",
    "from cerebro.tune.base import ModelSelection, update_model_results\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Cerebro Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "...\n",
    "work_dir = '/Users/zijian/Desktop/ucsd/cse234/project/cerebro-system/'\n",
    "backend = SparkBackend(spark_context=spark.sparkContext, num_workers=1)\n",
    "store = LocalStore(prefix_path=work_dir + 'test/')\n",
    "\n",
    "df = spark.read.format(\"libsvm\") \\\n",
    "    .option(\"numFeatures\", \"784\") \\\n",
    "    .load(\"/Users/zijian/Desktop/ucsd/cse234/project/mnist/mnist.scale\") \\\n",
    "\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "\n",
    "encoder = OneHotEncoderEstimator(dropLast=False)\n",
    "encoder.setInputCols([\"label\"])\n",
    "encoder.setOutputCols([\"label_OHE\"])\n",
    "\n",
    "encoder_model = encoder.fit(df)\n",
    "encoded = encoder_model.transform(df)\n",
    "\n",
    "feature_columns=['features']\n",
    "label_columns=['label_OHE']\n",
    "\n",
    "train_df, test_df = encoded.randomSplit([0.8, 0.2], 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e988695e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "48035"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c177a0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 28, 28, 1)\n",
      "(100, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-29 08:50:07.750218: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-11-29 08:50:07.750414: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Choice(name: \"learning_rate\", values: [0.01, 0.001], ordered: True, default: 0.01),\n",
       " Choice(name: \"batch_size\", values: [64, 128], ordered: True, default: 64),\n",
       " Fixed(name: conv_block_1/kernel_size, value: 3),\n",
       " Boolean(name: \"conv_block_1/separable\", default: False),\n",
       " Boolean(name: \"conv_block_1/max_pooling\", default: True),\n",
       " Choice(name: \"conv_block_1/dropout\", values: [0.0, 0.25, 0.5], ordered: True, default: 0),\n",
       " Fixed(name: conv_block_1/num_blocks, value: 1),\n",
       " Fixed(name: conv_block_1/num_layers, value: 3),\n",
       " Choice(name: \"conv_block_1/filters_0_0\", values: [16, 32, 64, 128, 256, 512], ordered: True, default: 32),\n",
       " Choice(name: \"conv_block_1/filters_0_1\", values: [16, 32, 64, 128, 256, 512], ordered: True, default: 32),\n",
       " Choice(name: \"conv_block_1/filters_0_2\", values: [16, 32, 64, 128, 256, 512], ordered: True, default: 32),\n",
       " Choice(name: \"classification_head_1/spatial_reduction_1/reduction_type\", values: ['flatten', 'global_max', 'global_avg'], ordered: False, default: flatten),\n",
       " Choice(name: \"classification_head_1/dropout\", values: [0.0, 0.25, 0.5], ordered: True, default: 0),\n",
       " Choice(name: \"optimizer\", values: ['adam', 'sgd', 'adam_weight_decay'], ordered: False, default: adam)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras_tuner import HyperParameters\n",
    "import autokeras as ak\n",
    "from cerebro.nas.hphpmodel import HyperHyperModel\n",
    "from keras_tuner.engine import hyperparameters\n",
    "\n",
    "\n",
    "img_shape = (28, 28, 1)\n",
    "num_classes = 10\n",
    "\n",
    "hp = HyperParameters()\n",
    "hp.Choice('learning_rate', values=[0.01,0.001])\n",
    "hp.Choice('batch_size', values=[64,128])\n",
    "\n",
    "input_node = ak.ImageInput()\n",
    "output_node = ak.ConvBlock(\n",
    "    kernel_size=hyperparameters.Fixed('kernel_size', value=3),\n",
    "    num_blocks=hyperparameters.Fixed('num_blocks', value=1),\n",
    "    num_layers=hyperparameters.Fixed('num_layers', value=3),\n",
    ")(input_node)\n",
    "output_node = ak.ClassificationHead()(output_node)\n",
    "am = HyperHyperModel(input_node, output_node, seed=2000)\n",
    "\n",
    "am.resource_bind(\n",
    "    backend=backend, \n",
    "    store=store,\n",
    "    feature_columns=feature_columns,\n",
    "    label_columns=label_columns,\n",
    "    evaluation_metric='accuracy', \n",
    ")\n",
    "\n",
    "am.tuner_bind(\n",
    "    tuner=\"randomsearch\", \n",
    "    hyperparameters=hp, \n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=20,\n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "x = np.array(train_df.select(feature_columns).head(100))\n",
    "y = np.array(train_df.select(label_columns).head(100))\n",
    "x = [x[:,i] for i in range(x.shape[1])]\n",
    "x = [r.reshape((-1, *img_shape)) for r in x]\n",
    "y = np.squeeze(y,1)\n",
    "\n",
    "print(x[0].shape)\n",
    "print(y.shape)\n",
    "\n",
    "dataset, validation_data = am._convert_to_dataset(\n",
    "    x=x, y=y, validation_data=None, batch_size=32\n",
    ")\n",
    "\n",
    "am._analyze_data(dataset)\n",
    "\n",
    "tuner = am.tuner\n",
    "tuner.hypermodel.hypermodel.set_fit_args(0.2, epochs=100)\n",
    "\n",
    "hp = tuner.oracle.get_space()\n",
    "tuner._prepare_model_IO(hp, dataset=dataset)\n",
    "tuner.hypermodel.build(hp)\n",
    "tuner.oracle.update_space(hp)\n",
    "hp = tuner.oracle.get_space()\n",
    "hp.space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "517273d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using keras model to train on existing cerebro backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7fbe31b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparkEstimator_f8a96d6897e0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=img_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "optimizer_fake = tf.keras.optimizers.Adam(lr=0.00001)\n",
    "loss = 'categorical_crossentropy'\n",
    "model.compile(optimizer=optimizer_fake, loss=loss, metrics=['accuracy'])\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.001)\n",
    "\n",
    "param = {\n",
    "    'model': model,\n",
    "    'optimizer': optimizer, # keras opt not str\n",
    "    'loss': 'categorical_crossentropy', # not sure\n",
    "    'metrics': ['accuracy'],\n",
    "    'batch_size': 64,\n",
    "    'custom_objects': tf.keras.utils.get_custom_objects()\n",
    "}\n",
    "\n",
    "est = SparkEstimator(\n",
    "        model=model,\n",
    "        optimizer=param['optimizer'],\n",
    "        loss=param['loss'],\n",
    "        metrics=param['metrics'],\n",
    "        batch_size=param['batch_size']\n",
    ")\n",
    "ms = tuner.model_selection\n",
    "est.setFeatureCols(ms.feature_cols)\n",
    "est.setLabelCols(ms.label_cols)\n",
    "est.setStore(ms.store)\n",
    "est.setVerbose(ms.verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95453f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-11-29 06:11:49, Num Partitions: 12\n",
      "CEREBRO => Time: 2021-11-29 06:11:49, Writing DataFrames\n",
      "CEREBRO => Time: 2021-11-29 06:11:49, Train Data Path: file:///Users/zijian/Desktop/ucsd/cse234/project/cerebro-system/test/intermediate_train_data\n",
      "CEREBRO => Time: 2021-11-29 06:11:49, Val Data Path: file:///Users/zijian/Desktop/ucsd/cse234/project/cerebro-system/test/intermediate_val_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-11-29 06:11:57, Train Partitions: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-11-29 06:12:14, Val Partitions: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-11-29 06:12:33, Train Rows: 38634\n",
      "CEREBRO => Time: 2021-11-29 06:12:33, Val Rows: 9401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:>                                                         (0 + 1) / 1]2021-11-29 06:12:51.851549: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-11-29 06:13:19.925389: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-29 06:13:19.931911: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:196] None of the MLIR optimization passes are enabled (registered 0 passes)\n",
      "WARNING:tensorflow:From /Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:2561: calling DatasetV2.from_generator (from tensorflow.python.data.ops.dataset_ops) with output_types is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use output_signature instead\n",
      "WARNING:tensorflow:From /Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:2561: calling DatasetV2.from_generator (from tensorflow.python.data.ops.dataset_ops) with output_shapes is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use output_signature instead\n",
      "WARNING:tensorflow:Expected a shuffled dataset but input dataset `x` is not shuffled. Please invoke `shuffle()` on input dataset.\n",
      "2021-11-29 06:13:20.711252: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "Train on 604 steps\n",
      "604/604 [==============================] - 18s 30ms/step - batch: 301.5000 - size: 1.0000 - loss: 0.6995 - accuracy: 0.7696 11s - batch: 136.5000 - size: 1.0000\n",
      "CEREBRO => Time: 2021-11-29 06:13:39, Model: model_0_1638195109, Mode: TRAIN, Initialization Time: 28.7895610332489, Training Time: 18.52852725982666, Finalization Time: 0.14176297187805176\n"
     ]
    }
   ],
   "source": [
    "_, _, metadata, _ = ms.backend.prepare_data(ms.store, train_df, ms.validation, label_columns=ms.label_cols, feature_columns=ms.feature_cols)\n",
    "ms.backend.initialize_workers()\n",
    "ms.backend.initialize_data_loaders(ms.store, None, ms.feature_cols + ms.label_cols)\n",
    "\n",
    "train_rel = ms.backend.train_for_one_epoch([est], ms.store, None, ms.feature_cols, ms.label_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "851c39bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using AK + cerebro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3ed04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(60000, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-29 08:50:18.002566: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "375/375 [==============================] - 27s 69ms/step - loss: 2.2252 - accuracy: 0.1491 - val_loss: 1.8650 - val_accuracy: 0.2660\n",
      "Epoch 2/5\n",
      "375/375 [==============================] - 25s 67ms/step - loss: 1.6387 - accuracy: 0.3862 - val_loss: 1.0759 - val_accuracy: 0.6172\n",
      "Epoch 3/5\n",
      "375/375 [==============================] - 28s 74ms/step - loss: 1.0351 - accuracy: 0.6290 - val_loss: 0.8704 - val_accuracy: 0.6962\n",
      "Epoch 4/5\n",
      "375/375 [==============================] - 28s 74ms/step - loss: 0.8796 - accuracy: 0.6851 - val_loss: 0.7582 - val_accuracy: 0.7445\n",
      "Epoch 5/5\n",
      "375/375 [==============================] - 28s 74ms/step - loss: 0.7789 - accuracy: 0.7332 - val_loss: 0.5533 - val_accuracy: 0.8332\n",
      "49b23e4059e1abb0c8fab15e2db406ad0\n",
      "Epoch 1/5\n",
      "188/188 [==============================] - 19s 100ms/step - loss: 2.3027 - accuracy: 0.1046 - val_loss: 2.3016 - val_accuracy: 0.1152\n",
      "Epoch 2/5\n",
      "188/188 [==============================] - 20s 105ms/step - loss: 2.3019 - accuracy: 0.1120 - val_loss: 2.3026 - val_accuracy: 0.1152\n",
      "Epoch 3/5\n",
      "188/188 [==============================] - 19s 99ms/step - loss: 2.3018 - accuracy: 0.1129 - val_loss: 2.3015 - val_accuracy: 0.1152\n",
      "Epoch 4/5\n",
      "188/188 [==============================] - 18s 98ms/step - loss: 2.3012 - accuracy: 0.1143 - val_loss: 2.3020 - val_accuracy: 0.1152\n",
      "Epoch 5/5\n",
      "188/188 [==============================] - 18s 98ms/step - loss: 2.3016 - accuracy: 0.1116 - val_loss: 2.3019 - val_accuracy: 0.1152\n",
      "49b23e4059e1abb0c8fab15e2db406ad1\n",
      "Epoch 1/5\n",
      "375/375 [==============================] - 28s 74ms/step - loss: 2.2355 - accuracy: 0.1372 - val_loss: 1.8016 - val_accuracy: 0.3363\n",
      "Epoch 2/5\n",
      "375/375 [==============================] - 28s 73ms/step - loss: 1.7288 - accuracy: 0.3640 - val_loss: 1.5018 - val_accuracy: 0.4695\n",
      "Epoch 3/5\n",
      "375/375 [==============================] - 28s 75ms/step - loss: 1.5018 - accuracy: 0.4550 - val_loss: 1.3091 - val_accuracy: 0.5518\n",
      "Epoch 4/5\n",
      "375/375 [==============================] - 28s 73ms/step - loss: 1.3088 - accuracy: 0.5398 - val_loss: 1.1240 - val_accuracy: 0.6183\n",
      "Epoch 5/5\n",
      "375/375 [==============================] - 29s 77ms/step - loss: 1.1285 - accuracy: 0.6053 - val_loss: 0.9730 - val_accuracy: 0.6838\n",
      "49b23e4059e1abb0c8fab15e2db406ad2\n",
      "Epoch 1/5\n",
      "188/188 [==============================] - 20s 103ms/step - loss: 2.2968 - accuracy: 0.1166 - val_loss: 2.1072 - val_accuracy: 0.2282\n",
      "Epoch 2/5\n",
      "188/188 [==============================] - 18s 98ms/step - loss: 2.0471 - accuracy: 0.2379 - val_loss: 1.9087 - val_accuracy: 0.2642\n",
      "Epoch 3/5\n",
      "188/188 [==============================] - 19s 99ms/step - loss: 1.8490 - accuracy: 0.3252 - val_loss: 1.5210 - val_accuracy: 0.4732\n",
      "Epoch 4/5\n",
      "188/188 [==============================] - 19s 102ms/step - loss: 1.5091 - accuracy: 0.4518 - val_loss: 1.3386 - val_accuracy: 0.5317\n",
      "Epoch 5/5\n",
      "188/188 [==============================] - 18s 98ms/step - loss: 1.3650 - accuracy: 0.5095 - val_loss: 1.2552 - val_accuracy: 0.5683\n",
      "49b23e4059e1abb0c8fab15e2db406ad3\n",
      "Epoch 1/5\n",
      "375/375 [==============================] - 1328s 4s/step - loss: 20.2202 - accuracy: 0.7783 - val_loss: 0.1781 - val_accuracy: 0.9462\n",
      "Epoch 2/5\n",
      "375/375 [==============================] - 1409s 4s/step - loss: 0.1707 - accuracy: 0.9503 - val_loss: 0.1126 - val_accuracy: 0.9662\n",
      "Epoch 3/5\n",
      "375/375 [==============================] - 1413s 4s/step - loss: 0.1237 - accuracy: 0.9621 - val_loss: 0.0901 - val_accuracy: 0.9733\n",
      "Epoch 4/5\n",
      "375/375 [==============================] - 1301s 3s/step - loss: 0.1011 - accuracy: 0.9685 - val_loss: 0.0976 - val_accuracy: 0.9717\n",
      "Epoch 5/5\n",
      "375/375 [==============================] - 1293s 3s/step - loss: 0.1002 - accuracy: 0.9703 - val_loss: 0.0916 - val_accuracy: 0.9750\n",
      "8629120c454fc5d454fab21e618f082a4\n",
      "Epoch 1/5\n",
      " 16/188 [=>............................] - ETA: 18:44 - loss: 77.9172 - accuracy: 0.2708"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print(x_train.shape)  # (60000, 28, 28)\n",
    "print(y_train.shape)  # (60000,)\n",
    "\n",
    "x_train = x_train[:30000]\n",
    "y_train = y_train[:30000]\n",
    "\n",
    "trial_rels = {}\n",
    "i = 0\n",
    "max_trial = 16\n",
    "ts = tuner.oracle.create_trials(max_trial, tuner.tuner_id)\n",
    "while i < max_trial:\n",
    "    trial = ts[i]\n",
    "    for lr in hp._hps['learning_rate'][0].values:\n",
    "        for bs in hp._hps['batch_size'][0].values:\n",
    "            if i < max_trial:\n",
    "                tuner._prepare_model_IO(trial.hyperparameters, dataset=dataset)\n",
    "                model = tuner.hypermodel.build(trial.hyperparameters)\n",
    "                tuner.adapt(model, dataset)\n",
    "                loss = 'categorical_crossentropy'\n",
    "                optimizer = tf.keras.optimizers.Adam(lr=lr)\n",
    "                model.compile(loss=loss, optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "                hist = model.fit(x_train, y_train, batch_size=bs, epochs=5, validation_split=0.2)\n",
    "                trial_rels[trial.trial_id + str(i)] = hist\n",
    "                print(trial.trial_id + str(i))\n",
    "                i = i + 1\n",
    "#         param = {\n",
    "#             'model': model,\n",
    "#             'optimizer': optimizer, # keras opt not str\n",
    "#             'loss': loss, # not sure\n",
    "#             'metrics': ['accuracy'],\n",
    "#             'batch_size': bs,\n",
    "#             'custom_objects': tf.keras.utils.get_custom_objects()\n",
    "#         }\n",
    "\n",
    "#         est = SparkEstimator(\n",
    "#             model=model,\n",
    "#             optimizer=param['optimizer'],\n",
    "#             loss=param['loss'],\n",
    "#             metrics=param['metrics'],\n",
    "#             batch_size=param['batch_size'],\n",
    "#             custom_objects=param['custom_objects']\n",
    "#         )\n",
    "#         ms = tuner.model_selection\n",
    "#         est.setFeatureCols(ms.feature_cols)\n",
    "#         est.setLabelCols(ms.label_cols)\n",
    "#         est.setStore(ms.store)\n",
    "#         est.setVerbose(ms.verbose)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f230ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a70ab41060107e4b3ec8b2f59726a0e6\n"
     ]
    }
   ],
   "source": [
    "ts = tuner.oracle.create_trials(4, tuner.tuner_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3a3070f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparkEstimator_b39a25611dd7"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = 'categorical_crossentropy'\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.001)\n",
    "\n",
    "param = {\n",
    "    'model': model,\n",
    "    'optimizer': optimizer, # keras opt not str\n",
    "    'loss': loss, # not sure\n",
    "    'metrics': ['accuracy'],\n",
    "    'batch_size': 64,\n",
    "    'custom_objects': tf.keras.utils.get_custom_objects()\n",
    "}\n",
    "\n",
    "est = SparkEstimator(\n",
    "    model=model,\n",
    "    optimizer=param['optimizer'],\n",
    "    loss=param['loss'],\n",
    "    metrics=param['metrics'],\n",
    "    batch_size=param['batch_size'],\n",
    "    custom_objects=param['custom_objects']\n",
    ")\n",
    "ms = tuner.model_selection\n",
    "est.setFeatureCols(ms.feature_cols)\n",
    "est.setLabelCols(ms.label_cols)\n",
    "est.setStore(ms.store)\n",
    "est.setVerbose(ms.verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55ea362d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-11-29 05:54:26, Num Partitions: 12\n",
      "CEREBRO => Time: 2021-11-29 05:54:26, Writing DataFrames\n",
      "CEREBRO => Time: 2021-11-29 05:54:26, Train Data Path: file:///Users/zijian/Desktop/ucsd/cse234/project/cerebro-system/test/intermediate_train_data\n",
      "CEREBRO => Time: 2021-11-29 05:54:26, Val Data Path: file:///Users/zijian/Desktop/ucsd/cse234/project/cerebro-system/test/intermediate_val_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-11-29 05:54:34, Train Partitions: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-11-29 05:54:52, Val Partitions: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-11-29 05:55:13, Train Rows: 38374\n",
      "CEREBRO => Time: 2021-11-29 05:55:13, Val Rows: 9661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:>                                                         (0 + 1) / 1]2021-11-29 05:55:58.561610: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-11-29 05:55:58.758475: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-29 05:55:58.763920: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:196] None of the MLIR optimization passes are enabled (registered 0 passes)\n",
      "WARNING:tensorflow:From /Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:2561: calling DatasetV2.from_generator (from tensorflow.python.data.ops.dataset_ops) with output_types is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use output_signature instead\n",
      "WARNING:tensorflow:From /Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:2561: calling DatasetV2.from_generator (from tensorflow.python.data.ops.dataset_ops) with output_shapes is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use output_signature instead\n",
      "WARNING:tensorflow:Expected a shuffled dataset but input dataset `x` is not shuffled. Please invoke `shuffle()` on input dataset.\n",
      "2021-11-29 05:55:59.476780: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "Train on 600 steps\n",
      "600/600 [==============================] - 212s 353ms/step - batch: 299.5000 - size: 1.0000 - loss: 2.3035 - accuracy: 0.1044\n",
      "CEREBRO => Time: 2021-11-29 05:59:31, Model: model_0_1638194059, Mode: TRAIN, Initialization Time: 0.8489768505096436, Training Time: 212.36887335777283, Finalization Time: 0.189497709274292\n",
      "[Stage 13:>                                                         (0 + 1) / 1]"
     ]
    }
   ],
   "source": [
    "_, _, metadata, _ = ms.backend.prepare_data(ms.store, train_df, ms.validation, label_columns=ms.label_cols, feature_columns=ms.feature_cols)\n",
    "ms.backend.initialize_workers()\n",
    "ms.backend.initialize_data_loaders(ms.store, None, ms.feature_cols + ms.label_cols)\n",
    "\n",
    "train_rel = ms.backend.train_for_one_epoch([est], ms.store, None, ms.feature_cols, ms.label_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "748380fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 13:>                                                         (0 + 1) / 1]"
     ]
    }
   ],
   "source": [
    "#Result \n",
    "\n",
    "estimator_results = {est.getRunId(): {}}\n",
    "update_model_results(estimator_results, train_rel)\n",
    "model = est.create_model(estimator_results[est.getRunId()], est.getRunId(), metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20a0d981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                16010     \n",
      "=================================================================\n",
      "Total params: 34,826\n",
      "Trainable params: 34,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = model.keras()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62eb3810",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c2cbbd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print(x_train.shape)  # (60000, 28, 28)\n",
    "print(y_train.shape)  # (60000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6db5ad2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/750 [===>..........................] - ETA: 3:14 - loss: 2.1722 - accuracy: 0.1737"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 13:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303/750 [===========>..................] - ETA: 2:23 - loss: 1.9582 - accuracy: 0.2654"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 13:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "473/750 [=================>............] - ETA: 1:32 - loss: 1.8156 - accuracy: 0.3260"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 13:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "645/750 [========================>.....] - ETA: 35s - loss: 1.6989 - accuracy: 0.3747"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 13:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 273s 364ms/step - loss: 1.6363 - accuracy: 0.4005 - val_loss: 0.4585 - val_accuracy: 0.8883\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x17c489990>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(lr=0.001)\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c79f8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 98/313 [========>.....................] - ETA: 10s - loss: 0.5420 - accuracy: 0.8578"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 13:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 16s 50ms/step - loss: 0.4501 - accuracy: 0.8905\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4500621557235718, 0.890500009059906]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:>                                                         (0 + 1) / 1]"
     ]
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aea7233d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6785b53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
