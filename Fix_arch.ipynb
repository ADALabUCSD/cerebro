{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6436ed30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/01 08:26:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-12-01 08:26:06, Running 1 Workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from cerebro.backend import SparkBackend\n",
    "from cerebro.keras import SparkEstimator\n",
    "\n",
    "# datas storage for intermediate data and model artifacts.\n",
    "from cerebro.storage import LocalStore, HDFSStore\n",
    "\n",
    "# Model selection/AutoML methods.\n",
    "from cerebro.tune import GridSearch, RandomSearch, TPESearch\n",
    "\n",
    "# Utility functions for specifying the search space.\n",
    "from cerebro.tune import hp_choice, hp_uniform, hp_quniform, hp_loguniform, hp_qloguniform\n",
    "from cerebro.tune.base import ModelSelection, update_model_results\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Cerebro Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "...\n",
    "work_dir = '/Users/zijian/Desktop/ucsd/cse234/project/cerebro-system/'\n",
    "backend = SparkBackend(spark_context=spark.sparkContext, num_workers=1)\n",
    "store = LocalStore(prefix_path=work_dir + 'test/')\n",
    "\n",
    "df = spark.read.format(\"libsvm\") \\\n",
    "    .option(\"numFeatures\", \"784\") \\\n",
    "    .load(\"/Users/zijian/Desktop/ucsd/cse234/project/mnist/mnist.scale\") \\\n",
    "\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "\n",
    "encoder = OneHotEncoderEstimator(dropLast=False)\n",
    "encoder.setInputCols([\"label\"])\n",
    "encoder.setOutputCols([\"label_OHE\"])\n",
    "\n",
    "encoder_model = encoder.fit(df)\n",
    "encoded = encoder_model.transform(df)\n",
    "\n",
    "feature_columns=['features']\n",
    "label_columns=['label_OHE']\n",
    "\n",
    "train_df, test_df = encoded.randomSplit([0.8, 0.2], 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e988695e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "48035"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c177a0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-12-01 08:26:11, Preparing Data\n",
      "CEREBRO => Time: 2021-12-01 08:26:11, Num Partitions: 12\n",
      "CEREBRO => Time: 2021-12-01 08:26:11, Writing DataFrames\n",
      "CEREBRO => Time: 2021-12-01 08:26:11, Train Data Path: file:///Users/zijian/Desktop/ucsd/cse234/project/cerebro-system/test/intermediate_train_data\n",
      "CEREBRO => Time: 2021-12-01 08:26:11, Val Data Path: file:///Users/zijian/Desktop/ucsd/cse234/project/cerebro-system/test/intermediate_val_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-12-01 08:26:18, Train Partitions: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-12-01 08:26:34, Val Partitions: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-12-01 08:26:52, Train Rows: 38457\n",
      "CEREBRO => Time: 2021-12-01 08:26:52, Val Rows: 9578\n",
      "CEREBRO => Time: 2021-12-01 08:26:52, Initializing Workers\n",
      "CEREBRO => Time: 2021-12-01 08:26:52, Initializing Data Loaders\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-01 08:27:11.827821: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-12-01 08:27:11.828024: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 28, 28, 1)\n",
      "(100, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Choice(name: \"optimizer\", values: ['adam'], ordered: False, default: adam),\n",
       " Choice(name: \"learning_rate\", values: [0.01, 0.001], ordered: True, default: 0.01),\n",
       " Choice(name: \"batch_size\", values: [64, 128], ordered: True, default: 64),\n",
       " Fixed(name: conv_block_1/kernel_size, value: 3),\n",
       " Boolean(name: \"conv_block_1/separable\", default: False),\n",
       " Boolean(name: \"conv_block_1/max_pooling\", default: True),\n",
       " Choice(name: \"conv_block_1/dropout\", values: [0.0, 0.25, 0.5], ordered: True, default: 0),\n",
       " Fixed(name: conv_block_1/num_blocks, value: 1),\n",
       " Fixed(name: conv_block_1/num_layers, value: 2),\n",
       " Choice(name: \"conv_block_1/filters_0_0\", values: [16, 32, 64, 128, 256, 512], ordered: True, default: 32),\n",
       " Choice(name: \"conv_block_1/filters_0_1\", values: [16, 32, 64, 128, 256, 512], ordered: True, default: 32),\n",
       " Choice(name: \"classification_head_1/spatial_reduction_1/reduction_type\", values: ['flatten', 'global_max', 'global_avg'], ordered: False, default: flatten),\n",
       " Choice(name: \"classification_head_1/dropout\", values: [0.0, 0.25, 0.5], ordered: True, default: 0)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras_tuner import HyperParameters\n",
    "import autokeras as ak\n",
    "from cerebro.nas.hphpmodel import HyperHyperModel\n",
    "from keras_tuner.engine import hyperparameters\n",
    "\n",
    "\n",
    "img_shape = (28, 28, 1)\n",
    "num_classes = 10\n",
    "\n",
    "hp = HyperParameters()\n",
    "hp.Choice('optimizer', values=['adam'])\n",
    "hp.Choice('learning_rate', values=[0.01,0.001])\n",
    "hp.Choice('batch_size', values=[64,128])\n",
    "\n",
    "input_node = ak.ImageInput()\n",
    "output_node = ak.ConvBlock(\n",
    "    kernel_size=hyperparameters.Fixed('kernel_size', value=3),\n",
    "    num_blocks=hyperparameters.Fixed('num_blocks', value=1),\n",
    "    num_layers=hyperparameters.Fixed('num_layers', value=2),\n",
    ")(input_node)\n",
    "output_node = ak.ClassificationHead()(output_node)\n",
    "am = HyperHyperModel(input_node, output_node, seed=2000)\n",
    "\n",
    "am.resource_bind(\n",
    "    backend=backend, \n",
    "    store=store,\n",
    "    feature_columns=feature_columns,\n",
    "    label_columns=label_columns,\n",
    "    evaluation_metric='accuracy', \n",
    ")\n",
    "\n",
    "am.tuner_bind(\n",
    "    tuner=\"randomsearch\", \n",
    "    hyperparameters=hp, \n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=5,\n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "_, _, meta_data, _ = am.sys_setup(train_df)\n",
    "\n",
    "x = np.array(train_df.select(feature_columns).head(100))\n",
    "y = np.array(train_df.select(label_columns).head(100))\n",
    "x = [x[:,i] for i in range(x.shape[1])]\n",
    "x = [r.reshape((-1, *img_shape)) for r in x]\n",
    "y = np.squeeze(y,1)\n",
    "\n",
    "print(x[0].shape)\n",
    "print(y.shape)\n",
    "\n",
    "dataset, validation_data = am._convert_to_dataset(\n",
    "    x=x, y=y, validation_data=None, batch_size=32\n",
    ")\n",
    "\n",
    "am._analyze_data(dataset)\n",
    "am.tuner.hyper_pipeline = None\n",
    "am.tuner.hypermodel.hyper_pipeline = None\n",
    "tuner = am.tuner\n",
    "tuner.hypermodel.hypermodel.set_fit_args(0.2, epochs=100)\n",
    "\n",
    "hp = tuner.oracle.get_space()\n",
    "tuner._prepare_model_IO(hp, dataset=dataset)\n",
    "tuner.hypermodel.build(hp)\n",
    "tuner.oracle.update_space(hp)\n",
    "hp = tuner.oracle.get_space()\n",
    "hp.space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3ebf2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                         (0 + 1) / 1]2021-12-01 08:27:36.351000: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-12-01 08:27:36.548786: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-01 08:27:36.554391: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:196] None of the MLIR optimization passes are enabled (registered 0 passes)\n",
      "WARNING:tensorflow:From /Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:2561: calling DatasetV2.from_generator (from tensorflow.python.data.ops.dataset_ops) with output_types is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use output_signature instead\n",
      "WARNING:tensorflow:From /Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:2561: calling DatasetV2.from_generator (from tensorflow.python.data.ops.dataset_ops) with output_shapes is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use output_signature instead\n",
      "WARNING:tensorflow:Expected a shuffled dataset but input dataset `x` is not shuffled. Please invoke `shuffle()` on input dataset.\n",
      "2021-12-01 08:27:37.332497: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "Train on 601 steps\n",
      "601/601 [==============================] - 30s 50ms/step - batch: 300.0000 - size: 1.0000 - loss: 2.3299 - accuracy: 0.0711\n",
      "CEREBRO => Time: 2021-12-01 08:28:08, Model: model_0_1638376032, Mode: TRAIN, Initialization Time: 0.9103348255157471, Training Time: 30.597962617874146, Finalization Time: 0.15317511558532715\n",
      "/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n",
      "[Stage 11:>                                                         (0 + 1) / 1]2021-12-01 08:28:14.704408: W tensorflow/core/framework/op_kernel.cc:1751] Invalid argument: ValueError: callback pyfunc_2 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 238, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_2 is not found\n",
      "\n",
      "\n",
      "2021-12-01 08:28:14.704806: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Invalid argument: ValueError: callback pyfunc_2 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 238, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_2 is not found\n",
      "\n",
      "\n",
      "\t [[{{node PyFunc}}]]\n",
      "CEREBRO => Time: 2021-12-01 08:28:14, Model: model_0_1638376032, Mode: VALID, Initialization Time: 0.5155820846557617, Training Time: 2.7415170669555664, Finalization Time: 0.09215903282165527\n",
      "WARNING:tensorflow:Expected a shuffled dataset but input dataset `x` is not shuffled. Please invoke `shuffle()` on input dataset.\n",
      "Train on 301 steps\n",
      "301/301 [==============================] - 25s 84ms/step - batch: 150.0000 - size: 1.0000 - loss: 2.3160 - accuracy: 0.1094\n",
      "2021-12-01 08:28:44.140151: W tensorflow/core/framework/op_kernel.cc:1751] Invalid argument: ValueError: callback pyfunc_5 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 238, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_5 is not found\n",
      "\n",
      "\n",
      "2021-12-01 08:28:44.140708: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Invalid argument: ValueError: callback pyfunc_5 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 238, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_5 is not found\n",
      "\n",
      "\n",
      "\t [[{{node PyFunc}}]]\n",
      "CEREBRO => Time: 2021-12-01 08:28:44, Model: model_1_1638376097, Mode: TRAIN, Initialization Time: 0.47771596908569336, Training Time: 25.949383020401, Finalization Time: 0.16098308563232422\n",
      "2021-12-01 08:28:50.847434: W tensorflow/core/framework/op_kernel.cc:1751] Invalid argument: ValueError: callback pyfunc_8 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 238, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_8 is not found\n",
      "\n",
      "\n",
      "2021-12-01 08:28:50.848478: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Invalid argument: ValueError: callback pyfunc_8 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 238, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_8 is not found\n",
      "\n",
      "\n",
      "\t [[{{node PyFunc}}]]\n",
      "CEREBRO => Time: 2021-12-01 08:28:50, Model: model_1_1638376097, Mode: VALID, Initialization Time: 0.5379068851470947, Training Time: 2.49333119392395, Finalization Time: 0.10311579704284668\n",
      "WARNING:tensorflow:Expected a shuffled dataset but input dataset `x` is not shuffled. Please invoke `shuffle()` on input dataset.\n",
      "Train on 601 steps\n",
      "601/601 [==============================] - 29s 48ms/step - batch: 300.0000 - size: 1.0000 - loss: 2.3009 - accuracy: 0.0940\n",
      "2021-12-01 08:29:23.787221: W tensorflow/core/framework/op_kernel.cc:1751] Invalid argument: ValueError: callback pyfunc_11 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 238, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_11 is not found\n",
      "\n",
      "\n",
      "2021-12-01 08:29:23.787707: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Invalid argument: ValueError: callback pyfunc_11 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 238, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_11 is not found\n",
      "\n",
      "\n",
      "\t [[{{node PyFunc}}]]\n",
      "CEREBRO => Time: 2021-12-01 08:29:23, Model: model_2_1638376133, Mode: TRAIN, Initialization Time: 0.349045991897583, Training Time: 29.36279821395874, Finalization Time: 0.16688299179077148\n",
      "2021-12-01 08:29:29.428891: W tensorflow/core/framework/op_kernel.cc:1751] Invalid argument: ValueError: callback pyfunc_14 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 238, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_14 is not found\n",
      "\n",
      "\n",
      "2021-12-01 08:29:29.429437: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Invalid argument: ValueError: callback pyfunc_14 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 238, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_14 is not found\n",
      "\n",
      "\n",
      "\t [[{{node PyFunc}}]]\n",
      "CEREBRO => Time: 2021-12-01 08:29:29, Model: model_2_1638376133, Mode: VALID, Initialization Time: 0.5393936634063721, Training Time: 2.7608792781829834, Finalization Time: 0.09109687805175781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected a shuffled dataset but input dataset `x` is not shuffled. Please invoke `shuffle()` on input dataset.\n",
      "Train on 301 steps\n",
      "301/301 [==============================] - 29s 95ms/step - batch: 150.0000 - size: 1.0000 - loss: 2.2831 - accuracy: 0.1034\n",
      "2021-12-01 08:30:01.978778: W tensorflow/core/framework/op_kernel.cc:1751] Invalid argument: ValueError: callback pyfunc_17 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 238, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_17 is not found\n",
      "\n",
      "\n",
      "2021-12-01 08:30:01.979171: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Invalid argument: ValueError: callback pyfunc_17 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 238, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_17 is not found\n",
      "\n",
      "\n",
      "\t [[{{node PyFunc}}]]\n",
      "CEREBRO => Time: 2021-12-01 08:30:01, Model: model_3_1638376172, Mode: TRAIN, Initialization Time: 0.34915781021118164, Training Time: 29.221812963485718, Finalization Time: 0.17416906356811523\n",
      "2021-12-01 08:30:07.577852: W tensorflow/core/framework/op_kernel.cc:1751] Invalid argument: ValueError: callback pyfunc_20 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 238, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_20 is not found\n",
      "\n",
      "\n",
      "2021-12-01 08:30:07.579577: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Invalid argument: ValueError: callback pyfunc_20 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 238, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_20 is not found\n",
      "\n",
      "\n",
      "\t [[{{node PyFunc}}]]\n",
      "CEREBRO => Time: 2021-12-01 08:30:07, Model: model_3_1638376172, Mode: VALID, Initialization Time: 0.5310709476470947, Training Time: 2.5314841270446777, Finalization Time: 0.12330484390258789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected a shuffled dataset but input dataset `x` is not shuffled. Please invoke `shuffle()` on input dataset.\n",
      "Train on 601 steps\n",
      "601/601 [==============================] - 78s 129ms/step - batch: 300.0000 - size: 1.0000 - loss: 2.3301 - accuracy: 0.0720\n",
      "2021-12-01 08:31:29.535384: W tensorflow/core/framework/op_kernel.cc:1751] Invalid argument: ValueError: callback pyfunc_23 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 238, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_23 is not found\n",
      "\n",
      "\n",
      "2021-12-01 08:31:29.535798: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Invalid argument: ValueError: callback pyfunc_23 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 238, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_23 is not found\n",
      "\n",
      "\n",
      "\t [[{{node PyFunc}}]]\n",
      "CEREBRO => Time: 2021-12-01 08:31:29, Model: model_4_1638376210, Mode: TRAIN, Initialization Time: 0.3670191764831543, Training Time: 78.38146686553955, Finalization Time: 0.18834614753723145\n",
      "2021-12-01 08:31:41.185699: W tensorflow/core/framework/op_kernel.cc:1751] Invalid argument: ValueError: callback pyfunc_26 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 238, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_26 is not found\n",
      "\n",
      "\n",
      "2021-12-01 08:31:41.186133: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Invalid argument: ValueError: callback pyfunc_26 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 238, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_26 is not found\n",
      "\n",
      "\n",
      "\t [[{{node PyFunc}}]]\n",
      "CEREBRO => Time: 2021-12-01 08:31:41, Model: model_4_1638376210, Mode: VALID, Initialization Time: 0.5558071136474609, Training Time: 7.551737070083618, Finalization Time: 0.09674811363220215\n"
     ]
    }
   ],
   "source": [
    "rel = tuner.fixed_arch_search(\n",
    "    hp=hp,\n",
    "    metadata=meta_data,\n",
    "    epoch=1,\n",
    "    x=dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "775e4c94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_0_1638376032': {'trial': <keras_tuner.engine.trial.Trial at 0x17e2f3b90>,\n",
       "  'train_loss': [2.3298865735233325],\n",
       "  'train_accuracy': [0.07110545039176941],\n",
       "  'val_loss': [2.3025798654556273],\n",
       "  'val_accuracy': [0.11312499642372131]},\n",
       " 'model_1_1638376097': {'trial': <keras_tuner.engine.trial.Trial at 0x17e2f3b90>,\n",
       "  'train_loss': [2.316047247066054],\n",
       "  'train_accuracy': [0.10942690819501877],\n",
       "  'val_loss': [2.301076924006144],\n",
       "  'val_accuracy': [0.1145833358168602]},\n",
       " 'model_2_1638376133': {'trial': <keras_tuner.engine.trial.Trial at 0x17e2f3b90>,\n",
       "  'train_loss': [2.3009028149127166],\n",
       "  'train_accuracy': [0.09395799040794373],\n",
       "  'val_loss': [2.269749337832133],\n",
       "  'val_accuracy': [0.19166666269302368]},\n",
       " 'model_3_1638376172': {'trial': <keras_tuner.engine.trial.Trial at 0x17e2f3b90>,\n",
       "  'train_loss': [2.283055753010848],\n",
       "  'train_accuracy': [0.10343126952648163],\n",
       "  'val_loss': [2.214654483795166],\n",
       "  'val_accuracy': [0.1432291716337204]},\n",
       " 'model_4_1638376210': {'trial': <keras_tuner.engine.trial.Trial at 0x17e41c810>,\n",
       "  'train_loss': [2.3301445093012094],\n",
       "  'train_accuracy': [0.07196339219808578],\n",
       "  'val_loss': [2.302228439648946],\n",
       "  'val_accuracy': [0.10270833224058151]}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                         (0 + 1) / 1]"
     ]
    }
   ],
   "source": [
    "# FIX archi results\n",
    "rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec23e10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面都是随便写的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "febfadd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.keras.losses.CategoricalCrossentropy object at 0x177bfd390>\n",
      "<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x178dad090>\n",
      "SparkEstimator_3697c2c63669__batch_size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 11:>                                                         (0 + 1) / 1]"
     ]
    }
   ],
   "source": [
    "trials = tuner.oracle.create_trials(1, tuner.tuner_id)\n",
    "trial = trials[0]\n",
    "est = tuner.trial_from_config_to_est(\n",
    "    trial=trial,\n",
    "    dataset=dataset,\n",
    "    learning_rate=0.01,\n",
    "    batch_size=32,\n",
    "    optimizer='adam'\n",
    ")\n",
    "print(est.getLoss())\n",
    "print(est.getOptimizer())\n",
    "print(est.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "408a0d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fafdd18349fece9cd58012d506bc58c9\n",
      "fafdd18349fece9cd58012d506bc58c9\n",
      "fafdd18349fece9cd58012d506bc58c9\n",
      "fafdd18349fece9cd58012d506bc58c9\n",
      "fafdd18349fece9cd58012d506bc58c9\n",
      "fafdd18349fece9cd58012d506bc58c9\n",
      "fafdd18349fece9cd58012d506bc58c9\n",
      "fafdd18349fece9cd58012d506bc58c9\n",
      "fafdd18349fece9cd58012d506bc58c9\n",
      "fafdd18349fece9cd58012d506bc58c9\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    trial = tuner.oracle.create_trial(tuner_id=tuner.tuner_id)\n",
    "    print(trial.trial_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c890d734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7be7ced5d9d778407c4d8ff964108a20\n",
      "1e126618dcc75cb311adee63175aa64c\n",
      "8efe73e94bbf42daba07bb352f28e029\n",
      "1464ca802ae32cc9a85f3155d756318d\n",
      "0be81600d47729f26ea1b05fe7614e7e\n",
      "4754450b0e3f957ea25364937c83c7f5\n",
      "203ef19ab78f7b1d808fe0d22619f194\n",
      "a05b22a0f2a01a45794ed0b5f78d70ed\n",
      "3e4462eebce9528663dcff6a1f1bfe80\n",
      "da42cd6d2c6c965b3f56d3fa13980fa3\n"
     ]
    }
   ],
   "source": [
    "# train using cerebro\n",
    "from cerebro.tune.base import update_model_results\n",
    "max_trial = 10\n",
    "i = 0\n",
    "ms = tuner.model_selection\n",
    "while i < max_trial:\n",
    "    trials = tuner.oracle.create_trials(1, tuner.tuner_id)\n",
    "    trial = trials[0]\n",
    "    for opt in hp._hps['optimizer'][0].values:\n",
    "        for lr in hp._hps['learning_rate'][0].values:\n",
    "            for bs in hp._hps['batch_size'][0].values:\n",
    "                if i < max_trial:\n",
    "                    estimator = tuner.trial_from_config_to_est(\n",
    "                        trial=trial,\n",
    "                        dataset=dataset,\n",
    "                        learning_rate=lr,\n",
    "                        batch_size=bs,\n",
    "                        optimizer=opt\n",
    "                    )\n",
    "                    for epoch in range(epochs):\n",
    "                    train_epoch = ms.backend.train_for_one_epoch([estimator], ms.store, None, ms.feature_cols, ms.label_cols)\n",
    "                    update_model_results(est_results, train_epoch)\n",
    "\n",
    "                    val_epoch = ms.backend.train_for_one_epoch(estimators, ms.store, dataset_idx, ms.feature_cols, ms.label_cols, is_train=False)\n",
    "                    update_model_results(est_results, val_epoch)\n",
    "                    self.on_epoch_end(estimators=estimators, est_resutls=est_results, epoch=epoch)\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27a2616c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Choice(name: \"optimizer\", values: ['adam'], ordered: False, default: adam),\n",
       " Choice(name: \"learning_rate\", values: [0.01, 0.001], ordered: True, default: 0.01),\n",
       " Choice(name: \"batch_size\", values: [64, 128], ordered: True, default: 64),\n",
       " Fixed(name: conv_block_1/kernel_size, value: 3),\n",
       " Boolean(name: \"conv_block_1/separable\", default: False),\n",
       " Boolean(name: \"conv_block_1/max_pooling\", default: True),\n",
       " Choice(name: \"conv_block_1/dropout\", values: [0.0, 0.25, 0.5], ordered: True, default: 0),\n",
       " Fixed(name: conv_block_1/num_blocks, value: 1),\n",
       " Fixed(name: conv_block_1/num_layers, value: 2),\n",
       " Choice(name: \"conv_block_1/filters_0_0\", values: [16, 32, 64, 128, 256, 512], ordered: True, default: 32),\n",
       " Choice(name: \"conv_block_1/filters_0_1\", values: [16, 32, 64, 128, 256, 512], ordered: True, default: 32),\n",
       " Choice(name: \"classification_head_1/spatial_reduction_1/reduction_type\", values: ['flatten', 'global_max', 'global_avg'], ordered: False, default: flatten),\n",
       " Choice(name: \"classification_head_1/dropout\", values: [0.0, 0.25, 0.5], ordered: True, default: 0)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner.oracle.hyperparameters.space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "517273d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using keras model to train on existing cerebro backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7fbe31b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparkEstimator_f8a96d6897e0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=img_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "optimizer_fake = tf.keras.optimizers.Adam(lr=0.00001)\n",
    "loss = 'categorical_crossentropy'\n",
    "model.compile(optimizer=optimizer_fake, loss=loss, metrics=['accuracy'])\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.001)\n",
    "\n",
    "param = {\n",
    "    'model': model,\n",
    "    'optimizer': optimizer, # keras opt not str\n",
    "    'loss': 'categorical_crossentropy', # not sure\n",
    "    'metrics': ['accuracy'],\n",
    "    'batch_size': 64,\n",
    "    'custom_objects': tf.keras.utils.get_custom_objects()\n",
    "}\n",
    "\n",
    "est = SparkEstimator(\n",
    "        model=model,\n",
    "        optimizer=param['optimizer'],\n",
    "        loss=param['loss'],\n",
    "        metrics=param['metrics'],\n",
    "        batch_size=param['batch_size']\n",
    ")\n",
    "ms = tuner.model_selection\n",
    "est.setFeatureCols(ms.feature_cols)\n",
    "est.setLabelCols(ms.label_cols)\n",
    "est.setStore(ms.store)\n",
    "est.setVerbose(ms.verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95453f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-11-29 06:11:49, Num Partitions: 12\n",
      "CEREBRO => Time: 2021-11-29 06:11:49, Writing DataFrames\n",
      "CEREBRO => Time: 2021-11-29 06:11:49, Train Data Path: file:///Users/zijian/Desktop/ucsd/cse234/project/cerebro-system/test/intermediate_train_data\n",
      "CEREBRO => Time: 2021-11-29 06:11:49, Val Data Path: file:///Users/zijian/Desktop/ucsd/cse234/project/cerebro-system/test/intermediate_val_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-11-29 06:11:57, Train Partitions: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-11-29 06:12:14, Val Partitions: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-11-29 06:12:33, Train Rows: 38634\n",
      "CEREBRO => Time: 2021-11-29 06:12:33, Val Rows: 9401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:>                                                         (0 + 1) / 1]2021-11-29 06:12:51.851549: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-11-29 06:13:19.925389: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-29 06:13:19.931911: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:196] None of the MLIR optimization passes are enabled (registered 0 passes)\n",
      "WARNING:tensorflow:From /Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:2561: calling DatasetV2.from_generator (from tensorflow.python.data.ops.dataset_ops) with output_types is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use output_signature instead\n",
      "WARNING:tensorflow:From /Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:2561: calling DatasetV2.from_generator (from tensorflow.python.data.ops.dataset_ops) with output_shapes is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use output_signature instead\n",
      "WARNING:tensorflow:Expected a shuffled dataset but input dataset `x` is not shuffled. Please invoke `shuffle()` on input dataset.\n",
      "2021-11-29 06:13:20.711252: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "Train on 604 steps\n",
      "604/604 [==============================] - 18s 30ms/step - batch: 301.5000 - size: 1.0000 - loss: 0.6995 - accuracy: 0.7696 11s - batch: 136.5000 - size: 1.0000\n",
      "CEREBRO => Time: 2021-11-29 06:13:39, Model: model_0_1638195109, Mode: TRAIN, Initialization Time: 28.7895610332489, Training Time: 18.52852725982666, Finalization Time: 0.14176297187805176\n"
     ]
    }
   ],
   "source": [
    "_, _, metadata, _ = ms.backend.prepare_data(ms.store, train_df, ms.validation, label_columns=ms.label_cols, feature_columns=ms.feature_cols)\n",
    "ms.backend.initialize_workers()\n",
    "ms.backend.initialize_data_loaders(ms.store, None, ms.feature_cols + ms.label_cols)\n",
    "\n",
    "train_rel = ms.backend.train_for_one_epoch([est], ms.store, None, ms.feature_cols, ms.label_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "851c39bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using AK + cerebro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3ed04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(60000, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-29 08:50:18.002566: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "375/375 [==============================] - 27s 69ms/step - loss: 2.2252 - accuracy: 0.1491 - val_loss: 1.8650 - val_accuracy: 0.2660\n",
      "Epoch 2/5\n",
      "375/375 [==============================] - 25s 67ms/step - loss: 1.6387 - accuracy: 0.3862 - val_loss: 1.0759 - val_accuracy: 0.6172\n",
      "Epoch 3/5\n",
      "375/375 [==============================] - 28s 74ms/step - loss: 1.0351 - accuracy: 0.6290 - val_loss: 0.8704 - val_accuracy: 0.6962\n",
      "Epoch 4/5\n",
      "375/375 [==============================] - 28s 74ms/step - loss: 0.8796 - accuracy: 0.6851 - val_loss: 0.7582 - val_accuracy: 0.7445\n",
      "Epoch 5/5\n",
      "375/375 [==============================] - 28s 74ms/step - loss: 0.7789 - accuracy: 0.7332 - val_loss: 0.5533 - val_accuracy: 0.8332\n",
      "49b23e4059e1abb0c8fab15e2db406ad0\n",
      "Epoch 1/5\n",
      "188/188 [==============================] - 19s 100ms/step - loss: 2.3027 - accuracy: 0.1046 - val_loss: 2.3016 - val_accuracy: 0.1152\n",
      "Epoch 2/5\n",
      "188/188 [==============================] - 20s 105ms/step - loss: 2.3019 - accuracy: 0.1120 - val_loss: 2.3026 - val_accuracy: 0.1152\n",
      "Epoch 3/5\n",
      "188/188 [==============================] - 19s 99ms/step - loss: 2.3018 - accuracy: 0.1129 - val_loss: 2.3015 - val_accuracy: 0.1152\n",
      "Epoch 4/5\n",
      "188/188 [==============================] - 18s 98ms/step - loss: 2.3012 - accuracy: 0.1143 - val_loss: 2.3020 - val_accuracy: 0.1152\n",
      "Epoch 5/5\n",
      "188/188 [==============================] - 18s 98ms/step - loss: 2.3016 - accuracy: 0.1116 - val_loss: 2.3019 - val_accuracy: 0.1152\n",
      "49b23e4059e1abb0c8fab15e2db406ad1\n",
      "Epoch 1/5\n",
      "375/375 [==============================] - 28s 74ms/step - loss: 2.2355 - accuracy: 0.1372 - val_loss: 1.8016 - val_accuracy: 0.3363\n",
      "Epoch 2/5\n",
      "375/375 [==============================] - 28s 73ms/step - loss: 1.7288 - accuracy: 0.3640 - val_loss: 1.5018 - val_accuracy: 0.4695\n",
      "Epoch 3/5\n",
      "375/375 [==============================] - 28s 75ms/step - loss: 1.5018 - accuracy: 0.4550 - val_loss: 1.3091 - val_accuracy: 0.5518\n",
      "Epoch 4/5\n",
      "375/375 [==============================] - 28s 73ms/step - loss: 1.3088 - accuracy: 0.5398 - val_loss: 1.1240 - val_accuracy: 0.6183\n",
      "Epoch 5/5\n",
      "375/375 [==============================] - 29s 77ms/step - loss: 1.1285 - accuracy: 0.6053 - val_loss: 0.9730 - val_accuracy: 0.6838\n",
      "49b23e4059e1abb0c8fab15e2db406ad2\n",
      "Epoch 1/5\n",
      "188/188 [==============================] - 20s 103ms/step - loss: 2.2968 - accuracy: 0.1166 - val_loss: 2.1072 - val_accuracy: 0.2282\n",
      "Epoch 2/5\n",
      "188/188 [==============================] - 18s 98ms/step - loss: 2.0471 - accuracy: 0.2379 - val_loss: 1.9087 - val_accuracy: 0.2642\n",
      "Epoch 3/5\n",
      "188/188 [==============================] - 19s 99ms/step - loss: 1.8490 - accuracy: 0.3252 - val_loss: 1.5210 - val_accuracy: 0.4732\n",
      "Epoch 4/5\n",
      "188/188 [==============================] - 19s 102ms/step - loss: 1.5091 - accuracy: 0.4518 - val_loss: 1.3386 - val_accuracy: 0.5317\n",
      "Epoch 5/5\n",
      "188/188 [==============================] - 18s 98ms/step - loss: 1.3650 - accuracy: 0.5095 - val_loss: 1.2552 - val_accuracy: 0.5683\n",
      "49b23e4059e1abb0c8fab15e2db406ad3\n",
      "Epoch 1/5\n",
      "375/375 [==============================] - 1328s 4s/step - loss: 20.2202 - accuracy: 0.7783 - val_loss: 0.1781 - val_accuracy: 0.9462\n",
      "Epoch 2/5\n",
      "375/375 [==============================] - 1409s 4s/step - loss: 0.1707 - accuracy: 0.9503 - val_loss: 0.1126 - val_accuracy: 0.9662\n",
      "Epoch 3/5\n",
      "375/375 [==============================] - 1413s 4s/step - loss: 0.1237 - accuracy: 0.9621 - val_loss: 0.0901 - val_accuracy: 0.9733\n",
      "Epoch 4/5\n",
      "375/375 [==============================] - 1301s 3s/step - loss: 0.1011 - accuracy: 0.9685 - val_loss: 0.0976 - val_accuracy: 0.9717\n",
      "Epoch 5/5\n",
      "375/375 [==============================] - 1293s 3s/step - loss: 0.1002 - accuracy: 0.9703 - val_loss: 0.0916 - val_accuracy: 0.9750\n",
      "8629120c454fc5d454fab21e618f082a4\n",
      "Epoch 1/5\n",
      " 16/188 [=>............................] - ETA: 18:44 - loss: 77.9172 - accuracy: 0.2708"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print(x_train.shape)  # (60000, 28, 28)\n",
    "print(y_train.shape)  # (60000,)\n",
    "\n",
    "x_train = x_train[:30000]\n",
    "y_train = y_train[:30000]\n",
    "\n",
    "trial_rels = {}\n",
    "i = 0\n",
    "max_trial = 16\n",
    "ts = tuner.oracle.create_trials(max_trial, tuner.tuner_id)\n",
    "while i < max_trial:\n",
    "    trial = ts[i]\n",
    "    for lr in hp._hps['learning_rate'][0].values:\n",
    "        for bs in hp._hps['batch_size'][0].values:\n",
    "            if i < max_trial:\n",
    "                tuner._prepare_model_IO(trial.hyperparameters, dataset=dataset)\n",
    "                model = tuner.hypermodel.build(trial.hyperparameters)\n",
    "                tuner.adapt(model, dataset)\n",
    "                loss = 'categorical_crossentropy'\n",
    "                optimizer = tf.keras.optimizers.Adam(lr=lr)\n",
    "                model.compile(loss=loss, optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "                hist = model.fit(x_train, y_train, batch_size=bs, epochs=5, validation_split=0.2)\n",
    "                trial_rels[trial.trial_id + str(i)] = hist\n",
    "                print(trial.trial_id + str(i))\n",
    "                i = i + 1\n",
    "#         param = {\n",
    "#             'model': model,\n",
    "#             'optimizer': optimizer, # keras opt not str\n",
    "#             'loss': loss, # not sure\n",
    "#             'metrics': ['accuracy'],\n",
    "#             'batch_size': bs,\n",
    "#             'custom_objects': tf.keras.utils.get_custom_objects()\n",
    "#         }\n",
    "\n",
    "#         est = SparkEstimator(\n",
    "#             model=model,\n",
    "#             optimizer=param['optimizer'],\n",
    "#             loss=param['loss'],\n",
    "#             metrics=param['metrics'],\n",
    "#             batch_size=param['batch_size'],\n",
    "#             custom_objects=param['custom_objects']\n",
    "#         )\n",
    "#         ms = tuner.model_selection\n",
    "#         est.setFeatureCols(ms.feature_cols)\n",
    "#         est.setLabelCols(ms.label_cols)\n",
    "#         est.setStore(ms.store)\n",
    "#         est.setVerbose(ms.verbose)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f230ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a70ab41060107e4b3ec8b2f59726a0e6\n"
     ]
    }
   ],
   "source": [
    "ts = tuner.oracle.create_trials(4, tuner.tuner_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3a3070f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparkEstimator_b39a25611dd7"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = 'categorical_crossentropy'\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.001)\n",
    "\n",
    "param = {\n",
    "    'model': model,\n",
    "    'optimizer': optimizer, # keras opt not str\n",
    "    'loss': loss, # not sure\n",
    "    'metrics': ['accuracy'],\n",
    "    'batch_size': 64,\n",
    "    'custom_objects': tf.keras.utils.get_custom_objects()\n",
    "}\n",
    "\n",
    "est = SparkEstimator(\n",
    "    model=model,\n",
    "    optimizer=param['optimizer'],\n",
    "    loss=param['loss'],\n",
    "    metrics=param['metrics'],\n",
    "    batch_size=param['batch_size'],\n",
    "    custom_objects=param['custom_objects']\n",
    ")\n",
    "ms = tuner.model_selection\n",
    "est.setFeatureCols(ms.feature_cols)\n",
    "est.setLabelCols(ms.label_cols)\n",
    "est.setStore(ms.store)\n",
    "est.setVerbose(ms.verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55ea362d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-11-29 05:54:26, Num Partitions: 12\n",
      "CEREBRO => Time: 2021-11-29 05:54:26, Writing DataFrames\n",
      "CEREBRO => Time: 2021-11-29 05:54:26, Train Data Path: file:///Users/zijian/Desktop/ucsd/cse234/project/cerebro-system/test/intermediate_train_data\n",
      "CEREBRO => Time: 2021-11-29 05:54:26, Val Data Path: file:///Users/zijian/Desktop/ucsd/cse234/project/cerebro-system/test/intermediate_val_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-11-29 05:54:34, Train Partitions: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-11-29 05:54:52, Val Partitions: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-11-29 05:55:13, Train Rows: 38374\n",
      "CEREBRO => Time: 2021-11-29 05:55:13, Val Rows: 9661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:>                                                         (0 + 1) / 1]2021-11-29 05:55:58.561610: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-11-29 05:55:58.758475: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-29 05:55:58.763920: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:196] None of the MLIR optimization passes are enabled (registered 0 passes)\n",
      "WARNING:tensorflow:From /Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:2561: calling DatasetV2.from_generator (from tensorflow.python.data.ops.dataset_ops) with output_types is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use output_signature instead\n",
      "WARNING:tensorflow:From /Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:2561: calling DatasetV2.from_generator (from tensorflow.python.data.ops.dataset_ops) with output_shapes is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use output_signature instead\n",
      "WARNING:tensorflow:Expected a shuffled dataset but input dataset `x` is not shuffled. Please invoke `shuffle()` on input dataset.\n",
      "2021-11-29 05:55:59.476780: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "Train on 600 steps\n",
      "600/600 [==============================] - 212s 353ms/step - batch: 299.5000 - size: 1.0000 - loss: 2.3035 - accuracy: 0.1044\n",
      "CEREBRO => Time: 2021-11-29 05:59:31, Model: model_0_1638194059, Mode: TRAIN, Initialization Time: 0.8489768505096436, Training Time: 212.36887335777283, Finalization Time: 0.189497709274292\n",
      "[Stage 13:>                                                         (0 + 1) / 1]"
     ]
    }
   ],
   "source": [
    "_, _, metadata, _ = ms.backend.prepare_data(ms.store, train_df, ms.validation, label_columns=ms.label_cols, feature_columns=ms.feature_cols)\n",
    "ms.backend.initialize_workers()\n",
    "ms.backend.initialize_data_loaders(ms.store, None, ms.feature_cols + ms.label_cols)\n",
    "\n",
    "train_rel = ms.backend.train_for_one_epoch([est], ms.store, None, ms.feature_cols, ms.label_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "748380fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 13:>                                                         (0 + 1) / 1]"
     ]
    }
   ],
   "source": [
    "#Result \n",
    "\n",
    "estimator_results = {est.getRunId(): {}}\n",
    "update_model_results(estimator_results, train_rel)\n",
    "model = est.create_model(estimator_results[est.getRunId()], est.getRunId(), metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20a0d981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                16010     \n",
      "=================================================================\n",
      "Total params: 34,826\n",
      "Trainable params: 34,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = model.keras()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62eb3810",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c2cbbd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print(x_train.shape)  # (60000, 28, 28)\n",
    "print(y_train.shape)  # (60000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6db5ad2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/750 [===>..........................] - ETA: 3:14 - loss: 2.1722 - accuracy: 0.1737"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 13:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303/750 [===========>..................] - ETA: 2:23 - loss: 1.9582 - accuracy: 0.2654"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 13:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "473/750 [=================>............] - ETA: 1:32 - loss: 1.8156 - accuracy: 0.3260"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 13:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "645/750 [========================>.....] - ETA: 35s - loss: 1.6989 - accuracy: 0.3747"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 13:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 273s 364ms/step - loss: 1.6363 - accuracy: 0.4005 - val_loss: 0.4585 - val_accuracy: 0.8883\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x17c489990>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(lr=0.001)\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c79f8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 98/313 [========>.....................] - ETA: 10s - loss: 0.5420 - accuracy: 0.8578"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 13:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 16s 50ms/step - loss: 0.4501 - accuracy: 0.8905\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4500621557235718, 0.890500009059906]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:>                                                         (0 + 1) / 1]"
     ]
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aea7233d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6785b53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
