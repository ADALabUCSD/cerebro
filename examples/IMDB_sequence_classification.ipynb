{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adopted from https://huggingface.co/transformers/custom_datasets.html#seq-imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-10-07 10:45:13--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
      "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 84125825 (80M) [application/x-gzip]\n",
      "Saving to: ‘aclImdb_v1.tar.gz’\n",
      "\n",
      "aclImdb_v1.tar.gz   100%[===================>]  80.23M  8.39MB/s    in 10s     \n",
      "\n",
      "2020-10-07 10:45:23 (7.90 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download data\n",
    "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from transformers import TFDistilBertForSequenceClassification, TFAlbertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "from cerebro.backend import SparkBackend\n",
    "from cerebro.keras import SparkEstimator\n",
    "from cerebro.storage import LocalStore\n",
    "from cerebro.tune import RandomSearch, GridSearch, hp_choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[3]\") \\\n",
    "    .appName(\"IMDB Sequence Classification\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_rdd = spark.sparkContext.wholeTextFiles(\"aclImdb/train/pos/*.txt\").map(lambda x: [x[0], 1])\n",
    "neg_rdd = spark.sparkContext.wholeTextFiles(\"aclImdb/train/neg/*.txt\").map(lambda x: [x[0], 0])\n",
    "merged_rdd = pos_rdd.union(neg_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_content(x):\n",
    "    with open(x[0][5:], 'r') as f:\n",
    "        return [\"\".join(f.readlines()), x[1]]\n",
    "\n",
    "merged_rdd = merged_rdd.map(lambda x: read_content(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = merged_rdd.toDF(['text', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_fraction = 0.01\n",
    "df = df.sample(False, sample_fraction, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def distilbert_tokenize(rows):\n",
    "    from transformers import DistilBertTokenizerFast\n",
    "    from pyspark.sql import Row\n",
    "    \n",
    "    tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for x in rows:\n",
    "        texts.append(x[0])\n",
    "        labels.append(x[1])\n",
    "    \n",
    "    encodings = tokenizer(texts, truncation=True, padding=True)\n",
    "    \n",
    "    for i in range(len(texts)):\n",
    "        yield Row(input_ids=encodings['input_ids'][i], attention_mask=encodings['attention_mask'][i],\n",
    "                  label=labels[i])\n",
    "    \n",
    "df = df.rdd.mapPartitions(lambda x: distilbert_tokenize(x)).toDF().cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+\n",
      "|      attention_mask|           input_ids|label|\n",
      "+--------------------+--------------------+-----+\n",
      "|[1, 1, 1, 1, 1, 1...|[101, 2023, 3185,...|    1|\n",
      "|[1, 1, 1, 1, 1, 1...|[101, 4315, 28681...|    1|\n",
      "|[1, 1, 1, 1, 1, 1...|[101, 3398, 2469,...|    1|\n",
      "|[1, 1, 1, 1, 1, 1...|[101, 1045, 1005,...|    1|\n",
      "|[1, 1, 1, 1, 1, 1...|[101, 1045, 3191,...|    1|\n",
      "|[1, 1, 1, 1, 1, 1...|[101, 1045, 2018,...|    1|\n",
      "|[1, 1, 1, 1, 1, 1...|[101, 1045, 3866,...|    1|\n",
      "|[1, 1, 1, 1, 1, 1...|[101, 3866, 2009,...|    1|\n",
      "|[1, 1, 1, 1, 1, 1...|[101, 2065, 2017,...|    1|\n",
      "|[1, 1, 1, 1, 1, 1...|[101, 2023, 16596...|    1|\n",
      "|[1, 1, 1, 1, 1, 1...|[101, 3374, 1010,...|    1|\n",
      "|[1, 1, 1, 1, 1, 1...|[101, 3666, 2023,...|    1|\n",
      "|[1, 1, 1, 1, 1, 1...|[101, 1045, 2347,...|    1|\n",
      "|[1, 1, 1, 1, 1, 1...|[101, 2054, 2062,...|    1|\n",
      "|[1, 1, 1, 1, 1, 1...|[101, 2600, 5024,...|    1|\n",
      "|[1, 1, 1, 1, 1, 1...|[101, 1045, 2428,...|    1|\n",
      "|[1, 1, 1, 1, 1, 1...|[101, 1045, 3866,...|    1|\n",
      "|[1, 1, 1, 1, 1, 1...|[101, 2465, 10301...|    1|\n",
      "|[1, 1, 1, 1, 1, 1...|[101, 1000, 2204,...|    1|\n",
      "|[1, 1, 1, 1, 1, 1...|[101, 2383, 11780...|    1|\n",
      "+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2020-10-07 11:19:17, Running 3 Workers (inferred from spark.default.parallelism)\n"
     ]
    }
   ],
   "source": [
    "backend = SparkBackend(spark_context=spark.sparkContext)\n",
    "store = LocalStore(\"/tmp\")\n",
    "\n",
    "search_space = {'lr': hp_choice([0.0001])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimator_gen_fn(params):\n",
    "    from tensorflow.keras.layers import Input, Flatten, Dense\n",
    "    from tensorflow.keras.models import Model\n",
    "    from transformers import TFDistilBertModel\n",
    "    \n",
    "    CUSTOM_OBJECTS = {'TFDistilBertModel': TFDistilBertModel}\n",
    "    \n",
    "    distilbert_model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "    input_ids = Input(shape=(512,), dtype=tf.int64, name='input_ids')\n",
    "    attention_mask = Input(shape=(512,), dtype=tf.float32, name='attention_mask')\n",
    "\n",
    "    bert = distilbert_model.distilbert(input_ids, attention_mask=attention_mask)[0]\n",
    "    \n",
    "    flat = Flatten()(bert)\n",
    "    classifier = Dense(units=1)(flat)\n",
    "    \n",
    "    model = Model(inputs=[input_ids, attention_mask], outputs=classifier)\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(lr=params['lr'])\n",
    "    loss = 'binary_crossentropy'\n",
    "\n",
    "    keras_estimator = SparkEstimator(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        loss=loss,\n",
    "        metrics=['acc'],\n",
    "        batch_size=10,\n",
    "        custom_objects=CUSTOM_OBJECTS)\n",
    "\n",
    "    return keras_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearch(backend, store, estimator_gen_fn, search_space, 5,\n",
    "                         validation=0.25, evaluation_metric='loss',\n",
    "                         feature_columns=['input_ids', 'attention_mask'],\n",
    "                         label_columns=['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2020-10-07 11:19:17, Preparing Data\n",
      "CEREBRO => Time: 2020-10-07 11:19:17, Num Partitions: 4\n",
      "CEREBRO => Time: 2020-10-07 11:19:17, Writing DataFrames\n",
      "CEREBRO => Time: 2020-10-07 11:19:17, Train Data Path: file:///tmp/intermediate_train_data\n",
      "CEREBRO => Time: 2020-10-07 11:19:17, Val Data Path: file:///tmp/intermediate_val_data\n",
      "CEREBRO => Time: 2020-10-07 11:20:01, Train Partitions: 3\n",
      "CEREBRO => Time: 2020-10-07 11:20:03, Val Partitions: 3\n",
      "CEREBRO => Time: 2020-10-07 11:20:04, Train Rows: 203\n",
      "CEREBRO => Time: 2020-10-07 11:20:04, Val Rows: 60\n",
      "CEREBRO => Time: 2020-10-07 11:20:04, Initializing Workers\n",
      "CEREBRO => Time: 2020-10-07 11:20:05, Initializing Data Loaders\n",
      "CEREBRO => Time: 2020-10-07 11:20:17, Launching Model Selection Workload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['activation_13', 'vocab_layer_norm', 'vocab_projector', 'vocab_transform']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = grid_search.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
