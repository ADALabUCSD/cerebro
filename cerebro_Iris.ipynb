{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ce6a85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/21 23:06:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/11/21 23:06:59 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "21/11/21 23:06:59 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-11-21 23:07:00, Running 1 Workers\n",
      "CEREBRO => Time: 2021-11-21 23:07:03, Preparing Data\n",
      "CEREBRO => Time: 2021-11-21 23:07:03, Num Partitions: 1\n",
      "CEREBRO => Time: 2021-11-21 23:07:03, Writing DataFrames\n",
      "CEREBRO => Time: 2021-11-21 23:07:03, Train Data Path: file:///Users/zijian/Desktop/ucsd/cse234/project/cerebro-system/experiments/intermediate_train_data\n",
      "CEREBRO => Time: 2021-11-21 23:07:03, Val Data Path: file:///Users/zijian/Desktop/ucsd/cse234/project/cerebro-system/experiments/intermediate_val_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-11-21 23:07:05, Train Partitions: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-11-21 23:07:08, Val Partitions: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-11-21 23:07:11, Train Rows: 89\n",
      "CEREBRO => Time: 2021-11-21 23:07:11, Val Rows: 27\n",
      "CEREBRO => Time: 2021-11-21 23:07:11, Initializing Workers\n",
      "CEREBRO => Time: 2021-11-21 23:07:11, Initializing Data Loaders\n",
      "CEREBRO => Time: 2021-11-21 23:07:11, Launching Model Selection Workload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-21 23:07:11.653309: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-11-21 23:07:11.653492: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[Stage 10:>                                                         (0 + 1) / 1]2021-11-21 23:07:11.862584: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-11-21 23:07:12.051972: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-21 23:07:12.063501: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:196] None of the MLIR optimization passes are enabled (registered 0 passes)\n",
      "WARNING:tensorflow:From /Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:2561: calling DatasetV2.from_generator (from tensorflow.python.data.ops.dataset_ops) with output_types is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use output_signature instead\n",
      "WARNING:tensorflow:From /Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:2561: calling DatasetV2.from_generator (from tensorflow.python.data.ops.dataset_ops) with output_shapes is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use output_signature instead\n",
      "WARNING:tensorflow:Expected a shuffled dataset but input dataset `x` is not shuffled. Please invoke `shuffle()` on input dataset.\n",
      "2021-11-21 23:07:12.789596: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "Train on 3 steps\n",
      "3/3 [==============================] - 0s 2ms/step - batch: 1.0000 - size: 1.0000 - loss: 1.5222 - accuracy: 0.4062\n",
      "CEREBRO => Time: 2021-11-21 23:07:13, Model: model_0_1637564831, Mode: TRAIN, Initialization Time: 0.8447589874267578, Training Time: 0.5535171031951904, Finalization Time: 0.20197319984436035\n",
      "/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n",
      "2021-11-21 23:07:16.919466: W tensorflow/core/framework/op_kernel.cc:1751] Invalid argument: ValueError: callback pyfunc_2 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 238, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_2 is not found\n",
      "\n",
      "\n",
      "2021-11-21 23:07:16.919894: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Invalid argument: ValueError: callback pyfunc_2 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 238, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_2 is not found\n",
      "\n",
      "\n",
      "\t [[{{node PyFunc}}]]\n",
      "CEREBRO => Time: 2021-11-21 23:07:16, Model: model_0_1637564831, Mode: VALID, Initialization Time: 0.8190269470214844, Training Time: 0.13908004760742188, Finalization Time: 0.08196806907653809\n",
      "WARNING:tensorflow:Expected a shuffled dataset but input dataset `x` is not shuffled. Please invoke `shuffle()` on input dataset.\n",
      "Train on 3 steps\n",
      "Epoch 2/2\n",
      "3/3 [==============================] - 0s 6ms/step - batch: 1.0000 - size: 1.0000 - loss: 1.5026 - accuracy: 0.3333    \n",
      "2021-11-21 23:07:21.252386: W tensorflow/core/framework/op_kernel.cc:1751] Invalid argument: ValueError: callback pyfunc_5 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 238, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_5 is not found\n",
      "\n",
      "\n",
      "2021-11-21 23:07:21.252767: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Invalid argument: ValueError: callback pyfunc_5 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 238, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_5 is not found\n",
      "\n",
      "\n",
      "\t [[{{node PyFunc}}]]\n",
      "CEREBRO => Time: 2021-11-21 23:07:21, Model: model_0_1637564831, Mode: TRAIN, Initialization Time: 0.7970280647277832, Training Time: 0.3285081386566162, Finalization Time: 0.20224380493164062\n",
      "2021-11-21 23:07:24.980628: W tensorflow/core/framework/op_kernel.cc:1751] Invalid argument: ValueError: callback pyfunc_8 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 238, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_8 is not found\n",
      "\n",
      "\n",
      "2021-11-21 23:07:24.981035: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Invalid argument: ValueError: callback pyfunc_8 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 238, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_8 is not found\n",
      "\n",
      "\n",
      "\t [[{{node PyFunc}}]]\n",
      "CEREBRO => Time: 2021-11-21 23:07:25, Model: model_0_1637564831, Mode: VALID, Initialization Time: 0.8053791522979736, Training Time: 0.13840579986572266, Finalization Time: 0.08572506904602051\n",
      "WARNING:tensorflow:Expected a shuffled dataset but input dataset `x` is not shuffled. Please invoke `shuffle()` on input dataset.\n",
      "Train on 3 steps\n",
      "Epoch 3/3\n",
      "3/3 [==============================] - 0s 6ms/step - batch: 1.0000 - size: 1.0000 - loss: 1.3758 - accuracy: 0.3333\n",
      "2021-11-21 23:07:29.326491: W tensorflow/core/framework/op_kernel.cc:1751] Invalid argument: ValueError: callback pyfunc_11 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 238, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_11 is not found\n",
      "\n",
      "\n",
      "2021-11-21 23:07:29.326888: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Invalid argument: ValueError: callback pyfunc_11 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 238, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_11 is not found\n",
      "\n",
      "\n",
      "\t [[{{node PyFunc}}]]\n",
      "CEREBRO => Time: 2021-11-21 23:07:29, Model: model_0_1637564831, Mode: TRAIN, Initialization Time: 0.8072400093078613, Training Time: 0.315579891204834, Finalization Time: 0.20221519470214844\n",
      "2021-11-21 23:07:33.047885: W tensorflow/core/framework/op_kernel.cc:1751] Invalid argument: ValueError: callback pyfunc_14 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 238, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_14 is not found\n",
      "\n",
      "\n",
      "2021-11-21 23:07:33.048268: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Invalid argument: ValueError: callback pyfunc_14 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 238, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_14 is not found\n",
      "\n",
      "\n",
      "\t [[{{node PyFunc}}]]\n",
      "CEREBRO => Time: 2021-11-21 23:07:33, Model: model_0_1637564831, Mode: VALID, Initialization Time: 0.7953777313232422, Training Time: 0.13637018203735352, Finalization Time: 0.0823817253112793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected a shuffled dataset but input dataset `x` is not shuffled. Please invoke `shuffle()` on input dataset.\n",
      "Train on 3 steps\n",
      "Epoch 4/4\n",
      "3/3 [==============================] - 0s 7ms/step - batch: 1.0000 - size: 1.0000 - loss: 1.2094 - accuracy: 0.4062\n",
      "2021-11-21 23:07:37.438778: W tensorflow/core/framework/op_kernel.cc:1751] Invalid argument: ValueError: callback pyfunc_17 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 238, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_17 is not found\n",
      "\n",
      "\n",
      "2021-11-21 23:07:37.439172: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Invalid argument: ValueError: callback pyfunc_17 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 238, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_17 is not found\n",
      "\n",
      "\n",
      "\t [[{{node PyFunc}}]]\n",
      "CEREBRO => Time: 2021-11-21 23:07:37, Model: model_0_1637564831, Mode: TRAIN, Initialization Time: 0.8189487457275391, Training Time: 0.3197910785675049, Finalization Time: 0.21707391738891602\n",
      "2021-11-21 23:07:41.129150: W tensorflow/core/framework/op_kernel.cc:1751] Invalid argument: ValueError: callback pyfunc_20 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 238, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_20 is not found\n",
      "\n",
      "\n",
      "2021-11-21 23:07:41.129547: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Invalid argument: ValueError: callback pyfunc_20 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 238, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_20 is not found\n",
      "\n",
      "\n",
      "\t [[{{node PyFunc}}]]\n",
      "CEREBRO => Time: 2021-11-21 23:07:41, Model: model_0_1637564831, Mode: VALID, Initialization Time: 0.796544075012207, Training Time: 0.13602805137634277, Finalization Time: 0.08300900459289551\n",
      "WARNING:tensorflow:Expected a shuffled dataset but input dataset `x` is not shuffled. Please invoke `shuffle()` on input dataset.\n",
      "Train on 3 steps\n",
      "Epoch 5/5\n",
      "3/3 [==============================] - 0s 6ms/step - batch: 1.0000 - size: 1.0000 - loss: 1.1999 - accuracy: 0.3333    \n",
      "2021-11-21 23:07:45.485708: W tensorflow/core/framework/op_kernel.cc:1751] Invalid argument: ValueError: callback pyfunc_23 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 238, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_23 is not found\n",
      "\n",
      "\n",
      "2021-11-21 23:07:45.486089: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Invalid argument: ValueError: callback pyfunc_23 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 238, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_23 is not found\n",
      "\n",
      "\n",
      "\t [[{{node PyFunc}}]]\n",
      "CEREBRO => Time: 2021-11-21 23:07:45, Model: model_0_1637564831, Mode: TRAIN, Initialization Time: 0.8028380870819092, Training Time: 0.32121706008911133, Finalization Time: 0.1983778476715088\n",
      "2021-11-21 23:07:49.234058: W tensorflow/core/framework/op_kernel.cc:1751] Invalid argument: ValueError: callback pyfunc_26 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 238, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_26 is not found\n",
      "\n",
      "\n",
      "2021-11-21 23:07:49.234473: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Invalid argument: ValueError: callback pyfunc_26 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 238, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_26 is not found\n",
      "\n",
      "\n",
      "\t [[{{node PyFunc}}]]\n",
      "CEREBRO => Time: 2021-11-21 23:07:49, Model: model_0_1637564831, Mode: VALID, Initialization Time: 0.8198912143707275, Training Time: 0.1365969181060791, Finalization Time: 0.08410191535949707\n",
      "WARNING:tensorflow:Expected a shuffled dataset but input dataset `x` is not shuffled. Please invoke `shuffle()` on input dataset.\n",
      "Train on 3 steps\n",
      "Epoch 6/6\n",
      "3/3 [==============================] - 0s 6ms/step - batch: 1.0000 - size: 1.0000 - loss: 1.1169 - accuracy: 0.3750\n",
      "2021-11-21 23:07:53.606315: W tensorflow/core/framework/op_kernel.cc:1751] Invalid argument: ValueError: callback pyfunc_29 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 238, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_29 is not found\n",
      "\n",
      "\n",
      "2021-11-21 23:07:53.606740: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Invalid argument: ValueError: callback pyfunc_29 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 238, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_29 is not found\n",
      "\n",
      "\n",
      "\t [[{{node PyFunc}}]]\n",
      "CEREBRO => Time: 2021-11-21 23:07:53, Model: model_0_1637564831, Mode: TRAIN, Initialization Time: 0.8209128379821777, Training Time: 0.32192111015319824, Finalization Time: 0.2210979461669922\n",
      "2021-11-21 23:07:57.300415: W tensorflow/core/framework/op_kernel.cc:1751] Invalid argument: ValueError: callback pyfunc_32 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 238, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_32 is not found\n",
      "\n",
      "\n",
      "2021-11-21 23:07:57.300829: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Invalid argument: ValueError: callback pyfunc_32 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 238, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_32 is not found\n",
      "\n",
      "\n",
      "\t [[{{node PyFunc}}]]\n",
      "CEREBRO => Time: 2021-11-21 23:07:57, Model: model_0_1637564831, Mode: VALID, Initialization Time: 0.8020389080047607, Training Time: 0.14114093780517578, Finalization Time: 0.08519387245178223\n",
      "WARNING:tensorflow:Expected a shuffled dataset but input dataset `x` is not shuffled. Please invoke `shuffle()` on input dataset.\n",
      "Train on 3 steps\n",
      "Epoch 7/7\n",
      "3/3 [==============================] - 0s 6ms/step - batch: 1.0000 - size: 1.0000 - loss: 1.1187 - accuracy: 0.0417    \n",
      "2021-11-21 23:08:01.663242: W tensorflow/core/framework/op_kernel.cc:1751] Invalid argument: ValueError: callback pyfunc_35 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 238, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_35 is not found\n",
      "\n",
      "\n",
      "2021-11-21 23:08:01.663624: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Invalid argument: ValueError: callback pyfunc_35 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 238, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_35 is not found\n",
      "\n",
      "\n",
      "\t [[{{node PyFunc}}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-11-21 23:08:01, Model: model_0_1637564831, Mode: TRAIN, Initialization Time: 0.8204758167266846, Training Time: 0.32283592224121094, Finalization Time: 0.19846820831298828\n",
      "2021-11-21 23:08:05.381015: W tensorflow/core/framework/op_kernel.cc:1751] Invalid argument: ValueError: callback pyfunc_38 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 238, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_38 is not found\n",
      "\n",
      "\n",
      "2021-11-21 23:08:05.381408: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Invalid argument: ValueError: callback pyfunc_38 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 238, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_38 is not found\n",
      "\n",
      "\n",
      "\t [[{{node PyFunc}}]]\n",
      "CEREBRO => Time: 2021-11-21 23:08:05, Model: model_0_1637564831, Mode: VALID, Initialization Time: 0.809783935546875, Training Time: 0.13623976707458496, Finalization Time: 0.08268117904663086\n",
      "WARNING:tensorflow:Expected a shuffled dataset but input dataset `x` is not shuffled. Please invoke `shuffle()` on input dataset.\n",
      "Train on 3 steps\n",
      "Epoch 8/8\n",
      "3/3 [==============================] - 0s 7ms/step - batch: 1.0000 - size: 1.0000 - loss: 1.0853 - accuracy: 0.3229\n",
      "2021-11-21 23:08:09.930864: W tensorflow/core/framework/op_kernel.cc:1751] Invalid argument: ValueError: callback pyfunc_41 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 238, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_41 is not found\n",
      "\n",
      "\n",
      "2021-11-21 23:08:09.931270: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Invalid argument: ValueError: callback pyfunc_41 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 238, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_41 is not found\n",
      "\n",
      "\n",
      "\t [[{{node PyFunc}}]]\n",
      "CEREBRO => Time: 2021-11-21 23:08:09, Model: model_0_1637564831, Mode: TRAIN, Initialization Time: 0.9152450561523438, Training Time: 0.37810301780700684, Finalization Time: 0.23030591011047363\n",
      "[Stage 10:>                                                         (0 + 1) / 1]2021-11-21 23:08:13.473258: W tensorflow/core/framework/op_kernel.cc:1751] Invalid argument: ValueError: callback pyfunc_44 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 238, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_44 is not found\n",
      "\n",
      "\n",
      "2021-11-21 23:08:13.473633: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Invalid argument: ValueError: callback pyfunc_44 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 238, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_44 is not found\n",
      "\n",
      "\n",
      "\t [[{{node PyFunc}}]]\n",
      "CEREBRO => Time: 2021-11-21 23:08:13, Model: model_0_1637564831, Mode: VALID, Initialization Time: 0.8107190132141113, Training Time: 0.13987302780151367, Finalization Time: 0.0830240249633789\n",
      "WARNING:tensorflow:Expected a shuffled dataset but input dataset `x` is not shuffled. Please invoke `shuffle()` on input dataset.\n",
      "Train on 3 steps\n",
      "Epoch 9/9\n",
      "3/3 [==============================] - 0s 6ms/step - batch: 1.0000 - size: 1.0000 - loss: 1.0791 - accuracy: 0.3229    \n",
      "2021-11-21 23:08:17.917385: W tensorflow/core/framework/op_kernel.cc:1751] Invalid argument: ValueError: callback pyfunc_47 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 238, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_47 is not found\n",
      "\n",
      "\n",
      "2021-11-21 23:08:17.917776: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Invalid argument: ValueError: callback pyfunc_47 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 238, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_47 is not found\n",
      "\n",
      "\n",
      "\t [[{{node PyFunc}}]]\n",
      "CEREBRO => Time: 2021-11-21 23:08:17, Model: model_0_1637564831, Mode: TRAIN, Initialization Time: 0.8797667026519775, Training Time: 0.3254063129425049, Finalization Time: 0.2231597900390625\n",
      "2021-11-21 23:08:21.752788: W tensorflow/core/framework/op_kernel.cc:1751] Invalid argument: ValueError: callback pyfunc_50 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 238, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_50 is not found\n",
      "\n",
      "\n",
      "2021-11-21 23:08:21.753184: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Invalid argument: ValueError: callback pyfunc_50 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 238, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_50 is not found\n",
      "\n",
      "\n",
      "\t [[{{node PyFunc}}]]\n",
      "CEREBRO => Time: 2021-11-21 23:08:21, Model: model_0_1637564831, Mode: VALID, Initialization Time: 0.9990508556365967, Training Time: 0.14569616317749023, Finalization Time: 0.08431696891784668\n",
      "WARNING:tensorflow:Expected a shuffled dataset but input dataset `x` is not shuffled. Please invoke `shuffle()` on input dataset.\n",
      "Train on 3 steps\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 0s 6ms/step - batch: 1.0000 - size: 1.0000 - loss: 1.0734 - accuracy: 0.3333\n",
      "2021-11-21 23:08:26.007847: W tensorflow/core/framework/op_kernel.cc:1751] Invalid argument: ValueError: callback pyfunc_53 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 238, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_53 is not found\n",
      "\n",
      "\n",
      "2021-11-21 23:08:26.008259: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Invalid argument: ValueError: callback pyfunc_53 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 238, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_53 is not found\n",
      "\n",
      "\n",
      "\t [[{{node PyFunc}}]]\n",
      "CEREBRO => Time: 2021-11-21 23:08:26, Model: model_0_1637564831, Mode: TRAIN, Initialization Time: 0.8869047164916992, Training Time: 0.32030320167541504, Finalization Time: 0.22698283195495605\n",
      "2021-11-21 23:08:29.713354: W tensorflow/core/framework/op_kernel.cc:1751] Invalid argument: ValueError: callback pyfunc_56 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 238, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_56 is not found\n",
      "\n",
      "\n",
      "2021-11-21 23:08:29.713736: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Invalid argument: ValueError: callback pyfunc_56 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 238, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_56 is not found\n",
      "\n",
      "\n",
      "\t [[{{node PyFunc}}]]\n",
      "CEREBRO => Time: 2021-11-21 23:08:29, Model: model_0_1637564831, Mode: VALID, Initialization Time: 0.8660061359405518, Training Time: 0.15679073333740234, Finalization Time: 0.08441686630249023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-11-21 23:08:32, Terminating Workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                         (0 + 1) / 1]2021-11-21 23:08:35.735822: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-11-21 23:08:35.736363: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-21 23:08:35.913791: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "[Stage 12:>                                                         (0 + 1) / 1]2021-11-21 23:08:38.619449: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-11-21 23:08:38.619853: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-21 23:08:38.786133: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|Species|    label_predicted|\n",
      "+-------+-------------------+\n",
      "|      0|0.30131030082702637|\n",
      "|      0|0.29584527015686035|\n",
      "|      0| 0.3063153028488159|\n",
      "|      0| 0.2972525656223297|\n",
      "|      1|0.26656144857406616|\n",
      "|      1| 0.2630133628845215|\n",
      "|      0| 0.2939305007457733|\n",
      "|      1|0.26326262950897217|\n",
      "|      0|   0.29635089635849|\n",
      "|      1|0.26233184337615967|\n",
      "+-------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from cerebro.backend import SparkBackend\n",
    "from cerebro.keras import SparkEstimator\n",
    "\n",
    "# datas storage for intermediate data and model artifacts.\n",
    "from cerebro.storage import LocalStore, HDFSStore\n",
    "\n",
    "# Model selection/AutoML methods.\n",
    "from cerebro.tune import GridSearch, RandomSearch, TPESearch\n",
    "\n",
    "# Utility functions for specifying the search space.\n",
    "from cerebro.tune import hp_choice, hp_uniform, hp_quniform, hp_loguniform, hp_qloguniform\n",
    "\n",
    "import tensorflow as tf\n",
    "# tf.config.run_functions_eagerly(True)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Cerebro Iris\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "...\n",
    "\n",
    "backend = SparkBackend(spark_context=spark.sparkContext, num_workers=1)\n",
    "store = LocalStore(prefix_path='/Users/zijian/Desktop/ucsd/cse234/project/cerebro-system/experiments')\n",
    "\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "\n",
    "df = spark.read.csv(\"/Users/zijian/Desktop/ucsd/cse234/project/cerebro-system/Iris_clean.csv\", header=True, inferSchema=True)\n",
    "\n",
    "encoder = OneHotEncoderEstimator(dropLast=False)\n",
    "encoder.setInputCols([\"Species\"])\n",
    "encoder.setOutputCols([\"Species_OHE\"])\n",
    "\n",
    "encoder_model = encoder.fit(df)\n",
    "encoded = encoder_model.transform(df)\n",
    "\n",
    "feature_columns=['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']\n",
    "label_columns=['Species_OHE']\n",
    "\n",
    "# Initialize input DataFrames.\n",
    "# You can download sample dataset from https://apache.googlesource.com/spark/+/master/data/mllib/sample_libsvm_data.txt\n",
    "\n",
    "train_df, test_df = encoded.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Define estimator generating function.\n",
    "# Input: Dictionary containing parameter values\n",
    "# Output: SparkEstimator\n",
    "def estimator_gen_fn(params):\n",
    "    inputs = [tf.keras.Input(shape=(1,)) for col in feature_columns]\n",
    "    embeddings1 = [tf.keras.layers.Dense(16, activation=tf.nn.relu)(input) for input in inputs]\n",
    "    embeddings2 = [tf.keras.layers.Dense(32, activation=tf.nn.relu)(input) for input in embeddings1]\n",
    "    combined = tf.keras.layers.Concatenate()(embeddings2)\n",
    "    output = tf.keras.layers.Dense(3, activation=tf.nn.softmax)(combined)\n",
    "    model = tf.keras.Model(inputs, output)\n",
    "\n",
    "#     inputs = tf.keras.Input(shape=(4,))\n",
    "#     output1 = tf.keras.layers.Dense(16, activation=tf.nn.relu)(inputs)\n",
    "#     output2 = tf.keras.layers.Dense(32, activation=tf.nn.relu)(output1)\n",
    "#     output = tf.keras.layers.Dense(3, activation=tf.nn.softmax)(output2)\n",
    "#     model = tf.keras.Model(inputs, output)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=params['lr'])\n",
    "    loss = 'categorical_crossentropy'\n",
    "\n",
    "    estimator = SparkEstimator(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        loss=loss,\n",
    "        metrics=['accuracy'],\n",
    "        batch_size=params['batch_size'])\n",
    "\n",
    "    return estimator\n",
    "\n",
    "# Define dictionary containing the parameter search space.\n",
    "search_space = {\n",
    "    'lr': hp_choice([0.01, 0.001, 0.0001]),\n",
    "    'batch_size': hp_quniform(16, 64, 16)\n",
    "}\n",
    "\n",
    "# Instantiate TPE (Tree of Parzan Estimators a.k.a., HyperOpt) model selection object.\n",
    "model_selection = TPESearch(\n",
    "    backend=backend, \n",
    "    store=store, \n",
    "    estimator_gen_fn=estimator_gen_fn, \n",
    "    search_space=search_space,\n",
    "    num_models=1, \n",
    "    num_epochs=10, \n",
    "    validation=0.25, \n",
    "    evaluation_metric='loss',\n",
    "    feature_columns=feature_columns,\n",
    "    label_columns=label_columns\n",
    ")\n",
    "\n",
    "# Perform model selection. Returns best model.\n",
    "model = model_selection.fit(train_df)\n",
    "\n",
    "# Inspect best model training history.\n",
    "model_history = model.get_history()\n",
    "\n",
    "# # Perform inference using the best model and Spark DataFrame.\n",
    "output_df = model.set_output_columns(['label_predicted']).transform(test_df)\n",
    "output_df.select('Species', 'label_predicted').show(n=10)\n",
    "\n",
    "# # Access all models.\n",
    "# all_models = model.get_all_models()\n",
    "# all_model_training_history = model.get_all_model_history()\n",
    "\n",
    "# # Convert the best model to Keras and perform inference using numpy data.\n",
    "# keras_model = model.keras()\n",
    "# pred = keras_model.predict([np.ones([1, 692], dtype=np.float32)])\n",
    "# # Save the keras checkpoint file.\n",
    "# keras_model.save(ckpt_path)\n",
    "\n",
    "# # Convert all the model to Keras.\n",
    "# all_models_keras = [m.keras() for m in all_models]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "586ef67c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Species_OHE': {'spark_data_type': pyspark.sql.types.BinaryType,\n",
       "  'is_sparse_vector_only': False,\n",
       "  'shape': None,\n",
       "  'intermediate_format': 'nochange',\n",
       "  'max_size': None},\n",
       " 'SepalLengthCm': {'spark_data_type': pyspark.sql.types.DoubleType,\n",
       "  'is_sparse_vector_only': False,\n",
       "  'shape': None,\n",
       "  'intermediate_format': 'nochange',\n",
       "  'max_size': None},\n",
       " 'SepalWidthCm': {'spark_data_type': pyspark.sql.types.DoubleType,\n",
       "  'is_sparse_vector_only': False,\n",
       "  'shape': None,\n",
       "  'intermediate_format': 'nochange',\n",
       "  'max_size': None},\n",
       " 'PetalLengthCm': {'spark_data_type': pyspark.sql.types.DoubleType,\n",
       "  'is_sparse_vector_only': False,\n",
       "  'shape': None,\n",
       "  'intermediate_format': 'nochange',\n",
       "  'max_size': None},\n",
       " 'PetalWidthCm': {'spark_data_type': pyspark.sql.types.DoubleType,\n",
       "  'is_sparse_vector_only': False,\n",
       "  'shape': None,\n",
       "  'intermediate_format': 'nochange',\n",
       "  'max_size': None}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_best_model()._get_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e58d11eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/21 23:29:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/11/21 23:29:20 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "21/11/21 23:29:20 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-11-21 23:29:21, Running 1 Workers\n",
      "CEREBRO => Time: 2021-11-21 23:29:24, Num Partitions: 1\n",
      "CEREBRO => Time: 2021-11-21 23:29:24, Writing DataFrames\n",
      "CEREBRO => Time: 2021-11-21 23:29:24, Train Data Path: file:///Users/zijian/Desktop/ucsd/cse234/project/cerebro-system/experiments/intermediate_train_data\n",
      "CEREBRO => Time: 2021-11-21 23:29:24, Val Data Path: file:///Users/zijian/Desktop/ucsd/cse234/project/cerebro-system/experiments/intermediate_val_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 4:>                                                          (0 + 1) / 1]\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PetalLengthCm': {'spark_data_type': <class 'pyspark.sql.types.DoubleType'>, 'is_sparse_vector_only': False, 'shape': 1, 'intermediate_format': 'nochange', 'max_size': 1}, 'PetalWidthCm': {'spark_data_type': <class 'pyspark.sql.types.DoubleType'>, 'is_sparse_vector_only': False, 'shape': 1, 'intermediate_format': 'nochange', 'max_size': 1}, 'SepalLengthCm': {'spark_data_type': <class 'pyspark.sql.types.DoubleType'>, 'is_sparse_vector_only': False, 'shape': 1, 'intermediate_format': 'nochange', 'max_size': 1}, 'SepalWidthCm': {'spark_data_type': <class 'pyspark.sql.types.DoubleType'>, 'is_sparse_vector_only': False, 'shape': 1, 'intermediate_format': 'nochange', 'max_size': 1}, 'Species_OHE': {'spark_data_type': <class 'pyspark.sql.types.ArrayType'>, 'is_sparse_vector_only': False, 'shape': 3, 'intermediate_format': 'nochange', 'max_size': 3}}\n",
      "CEREBRO => Time: 2021-11-21 23:29:26, Train Partitions: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-11-21 23:29:29, Val Partitions: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-11-21 23:29:32, Train Rows: 96\n",
      "CEREBRO => Time: 2021-11-21 23:29:32, Val Rows: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:>                                                         (0 + 1) / 1]"
     ]
    }
   ],
   "source": [
    "from cerebro.backend import SparkBackend\n",
    "from cerebro.keras import SparkEstimator\n",
    "\n",
    "# datas storage for intermediate data and model artifacts.\n",
    "from cerebro.storage import LocalStore, HDFSStore\n",
    "\n",
    "# Model selection/AutoML methods.\n",
    "from cerebro.tune import GridSearch, RandomSearch, TPESearch\n",
    "\n",
    "# Utility functions for specifying the search space.\n",
    "from cerebro.tune import hp_choice, hp_uniform, hp_quniform, hp_loguniform, hp_qloguniform\n",
    "\n",
    "import tensorflow as tf\n",
    "# tf.config.run_functions_eagerly(True)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Cerebro Iris\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "...\n",
    "\n",
    "backend = SparkBackend(spark_context=spark.sparkContext, num_workers=1)\n",
    "store = LocalStore(prefix_path='/Users/zijian/Desktop/ucsd/cse234/project/cerebro-system/experiments')\n",
    "\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "\n",
    "df = spark.read.csv(\"/Users/zijian/Desktop/ucsd/cse234/project/cerebro-system/Iris_clean.csv\", header=True, inferSchema=True)\n",
    "\n",
    "encoder = OneHotEncoderEstimator(dropLast=False)\n",
    "encoder.setInputCols([\"Species\"])\n",
    "encoder.setOutputCols([\"Species_OHE\"])\n",
    "\n",
    "encoder_model = encoder.fit(df)\n",
    "encoded = encoder_model.transform(df)\n",
    "\n",
    "feature_columns=['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']\n",
    "label_columns=['Species_OHE']\n",
    "\n",
    "# Initialize input DataFrames.\n",
    "# You can download sample dataset from https://apache.googlesource.com/spark/+/master/data/mllib/sample_libsvm_data.txt\n",
    "\n",
    "train_df, test_df = encoded.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Define estimator generating function.\n",
    "# Input: Dictionary containing parameter values\n",
    "# Output: SparkEstimator\n",
    "def estimator_gen_fn(params):\n",
    "    inputs = [tf.keras.Input(shape=(1,)) for col in feature_columns]\n",
    "    embeddings = [tf.keras.layers.Dense(16, activation=tf.nn.relu)(input) for input in inputs]\n",
    "    combined = tf.keras.layers.Concatenate()(embeddings)\n",
    "    output = tf.keras.layers.Dense(3, activation=tf.nn.softmax)(combined)\n",
    "    model = tf.keras.Model(inputs, output)\n",
    "\n",
    "#     inputs = tf.keras.Input(shape=(4,))\n",
    "#     output1 = tf.keras.layers.Dense(16, activation=tf.nn.relu)(inputs)\n",
    "#     output2 = tf.keras.layers.Dense(32, activation=tf.nn.relu)(output1)\n",
    "#     output = tf.keras.layers.Dense(3, activation=tf.nn.softmax)(output2)\n",
    "#     model = tf.keras.Model(inputs, output)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=params['lr'])\n",
    "    loss = 'categorical_crossentropy'\n",
    "\n",
    "    estimator = SparkEstimator(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        loss=loss,\n",
    "        metrics=['accuracy'],\n",
    "        batch_size=params['batch_size'])\n",
    "\n",
    "    return estimator\n",
    "\n",
    "# Define dictionary containing the parameter search space.\n",
    "search_space = {\n",
    "    'lr': hp_choice([0.01, 0.001, 0.0001]),\n",
    "    'batch_size': hp_quniform(16, 64, 16)\n",
    "}\n",
    "\n",
    "# Instantiate TPE (Tree of Parzan Estimators a.k.a., HyperOpt) model selection object.\n",
    "model_selection = TPESearch(\n",
    "    backend=backend, \n",
    "    store=store, \n",
    "    estimator_gen_fn=estimator_gen_fn, \n",
    "    search_space=search_space,\n",
    "    num_models=1, \n",
    "    num_epochs=10, \n",
    "    validation=0.25, \n",
    "    evaluation_metric='loss',\n",
    "    feature_columns=feature_columns,\n",
    "    label_columns=label_columns\n",
    ")\n",
    "\n",
    "_, _, metadata, _ = model_selection.backend.prepare_data(model_selection.store, train_df, model_selection.validation, label_columns=model_selection.label_cols, feature_columns=model_selection.feature_cols)\n",
    "\n",
    "model_selection.backend.initialize_workers()\n",
    "\n",
    "model_selection.backend.initialize_data_loaders(model_selection.store, None, model_selection.feature_cols + model_selection.label_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01d4f60a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Species_OHE': {'spark_data_type': pyspark.sql.types.BinaryType,\n",
       "  'is_sparse_vector_only': False,\n",
       "  'shape': None,\n",
       "  'intermediate_format': 'nochange',\n",
       "  'max_size': None},\n",
       " 'SepalLengthCm': {'spark_data_type': pyspark.sql.types.DoubleType,\n",
       "  'is_sparse_vector_only': False,\n",
       "  'shape': None,\n",
       "  'intermediate_format': 'nochange',\n",
       "  'max_size': None},\n",
       " 'SepalWidthCm': {'spark_data_type': pyspark.sql.types.DoubleType,\n",
       "  'is_sparse_vector_only': False,\n",
       "  'shape': None,\n",
       "  'intermediate_format': 'nochange',\n",
       "  'max_size': None},\n",
       " 'PetalLengthCm': {'spark_data_type': pyspark.sql.types.DoubleType,\n",
       "  'is_sparse_vector_only': False,\n",
       "  'shape': None,\n",
       "  'intermediate_format': 'nochange',\n",
       "  'max_size': None},\n",
       " 'PetalWidthCm': {'spark_data_type': pyspark.sql.types.DoubleType,\n",
       "  'is_sparse_vector_only': False,\n",
       "  'shape': None,\n",
       "  'intermediate_format': 'nochange',\n",
       "  'max_size': None}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:>                                                         (0 + 1) / 1]"
     ]
    }
   ],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "555c2b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'SepalLengthCm': {'spark_data_type': pyspark.sql.types.DoubleType,\n",
       "  'is_sparse_vector_only': False,\n",
       "  'shape': 1,\n",
       "  'intermediate_format': 'nochange',\n",
       "  'max_size': 1},\n",
       " 'SepalWidthCm': {'spark_data_type': pyspark.sql.types.DoubleType,\n",
       "  'is_sparse_vector_only': False,\n",
       "  'shape': 1,\n",
       "  'intermediate_format': 'nochange',\n",
       "  'max_size': 1},\n",
       " 'PetalLengthCm': {'spark_data_type': pyspark.sql.types.DoubleType,\n",
       "  'is_sparse_vector_only': False,\n",
       "  'shape': 1,\n",
       "  'intermediate_format': 'nochange',\n",
       "  'max_size': 1},\n",
       " 'PetalWidthCm': {'spark_data_type': pyspark.sql.types.DoubleType,\n",
       "  'is_sparse_vector_only': False,\n",
       "  'shape': 1,\n",
       "  'intermediate_format': 'nochange',\n",
       "  'max_size': 1},\n",
       " 'Species': {'spark_data_type': pyspark.sql.types.IntegerType,\n",
       "  'is_sparse_vector_only': False,\n",
       "  'shape': 1,\n",
       "  'intermediate_format': 'nochange',\n",
       "  'max_size': 1},\n",
       " 'Species_OHE': {'spark_data_type': pyspark.ml.linalg.SparseVector,\n",
       "  'is_sparse_vector_only': True,\n",
       "  'shape': 3,\n",
       "  'intermediate_format': 'custom_sparse_format',\n",
       "  'max_size': 1}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:>                                                         (0 + 1) / 1]"
     ]
    }
   ],
   "source": [
    "from cerebro.backend.spark.util import _get_metadata\n",
    "\n",
    "_get_metadata(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b68fff84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-21 23:02:51.155902: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-11-21 23:02:51.156214: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\r",
      "[Stage 10:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.1>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=0.1>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.3>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.0>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 0., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.2>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=0.2>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.8>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.0>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 0., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.3>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=0.2>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.4>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.0>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 0., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.3>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=0.2>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.7>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.2>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 0., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.3>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=0.2>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.5>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.5>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 0., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.3>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=0.3>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.0>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.5>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 0., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.4>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=0.1>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.8>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.0>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 0., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.4>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=0.2>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.4>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.9>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 0., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.4>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=0.2>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.9>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.0>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 0., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.4>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=0.2>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.2>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.4>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 0., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.4>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=0.2>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.5>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.2>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 0., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.4>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=0.3>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.8>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.0>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 0., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.5>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=0.1>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.9>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.1>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 0., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.5>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=0.1>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.2>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.1>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 0., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.5>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=0.2>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.0>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.4>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 0., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.5>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=0.2>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.1>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.4>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 0., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.5>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=0.2>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.4>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.7>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 0., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.5>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=0.3>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.1>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.8>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 0., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.5>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=0.4>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.1>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.7>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 0., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.5>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=0.4>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.4>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.4>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 0., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.6>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=0.2>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.7>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.2>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 0., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.6>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=0.2>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.8>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.4>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 0., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.6>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=0.2>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.0>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.0>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 0., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.6>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=0.2>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.1>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.8>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 0., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.6>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=0.6>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.0>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.5>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 0., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.7>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=0.3>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.7>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.8>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 0., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.7>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=0.4>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.4>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.9>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 0., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.0>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.1>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.1>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.5>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.3>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.0>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.0>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.3>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.5>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.0>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.0>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.0>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.5>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.0>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.7>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.6>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.6>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.3>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.6>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.9>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.9>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.1>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.6>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.5>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.9>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.2>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.8>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.7>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.0>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.0>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.0>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.2>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.0>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.3>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.5>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.3>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.0>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.3>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.1>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.8>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.1>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.3>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.6>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.0>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.1>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.3>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.7>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.8>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.2>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.3>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.6>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.7>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.2>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.5>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.9>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.0>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.3>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.3>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.2>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.9>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.3>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.3>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.4>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.9>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.4>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.3>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.3>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.3>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.4>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.4>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.7>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.1>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.5>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.3>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.7>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.8>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.5>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.5>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.4>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.0>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.5>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.5>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.2>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.2>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.5>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.5>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.4>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.2>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.5>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.6>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.0>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.4>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.6>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.3>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.6>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.9>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.6>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.4>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.1>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.0>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.7>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.2>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.1>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.8>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.7>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.4>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=7.0>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.2>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.7>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.5>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.7>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.1>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.8>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.8>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.9>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.2>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.8>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.8>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.2>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.8>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 0., 1.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.9>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.5>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.3>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.5>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.9>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.8>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.1>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.0>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 0., 1.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.9>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.0>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.6>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.8>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 0., 1.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.0>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.5>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.0>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.2>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 0., 1.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.0>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.7>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.7>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.0>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.0>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.9>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.3>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.5>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 0., 1.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.0>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.0>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.7>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.5>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 0., 1.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.1>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.6>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.0>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.7>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.1>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.9>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.8>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.7>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 0., 1.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.1>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.3>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.9>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.1>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 0., 1.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.2>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.0>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.5>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.0>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 0., 1.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.3>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.9>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.4>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.7>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 0., 1.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.3>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.3>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.4>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.2>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 0., 1.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.4>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.1>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.9>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.1>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 0., 1.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.4>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.3>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.2>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.4>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 0., 1.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.6>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.4>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.1>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.6>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 0., 1.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.6>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.8>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.3>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.9>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 0., 1.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.6>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.2>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.4>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.8>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 0., 1.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.6>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.4>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.7>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.1>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 0., 1.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.7>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.1>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.7>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.3>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 0., 1.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.7>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.3>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.9>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.2>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 0., 1.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.7>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.5>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.7>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.3>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 0., 1.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.8>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.2>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.5>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.0>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 0., 1.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.9>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.3>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.8>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.2>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 0., 1.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.1>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.9>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=7.4>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.8>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 0., 1.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.1>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.5>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=7.2>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.6>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 0., 1.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.4>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.0>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=7.9>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.8>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 0., 1.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.7>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.0>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=7.7>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.8>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 0., 1.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.7>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.2>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=7.7>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.8>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 0., 1.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.9>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.3>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=7.7>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.6>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 0., 1.])>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-21 23:02:51.604769: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "[Stage 10:>                                                         (0 + 1) / 1]"
     ]
    }
   ],
   "source": [
    "from petastorm import make_reader\n",
    "\n",
    "from petastorm.tf_utils import make_petastorm_dataset\n",
    "\n",
    "with make_reader('file:///Users/zijian/Desktop/ucsd/cse234/project/cerebro-system/experiments/intermediate_train_data') as reader:\n",
    "    dataset = make_petastorm_dataset(reader)\n",
    "    for ele in dataset:\n",
    "        print(ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b7b5791",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(SepalLengthCm=4.4, SepalWidthCm=2.9, PetalLengthCm=1.4, PetalWidthCm=0.2, Species=0, Species_OHE=SparseVector(3, {0: 1.0})),\n",
       " Row(SepalLengthCm=4.4, SepalWidthCm=3.0, PetalLengthCm=1.3, PetalWidthCm=0.2, Species=0, Species_OHE=SparseVector(3, {0: 1.0})),\n",
       " Row(SepalLengthCm=4.4, SepalWidthCm=3.2, PetalLengthCm=1.3, PetalWidthCm=0.2, Species=0, Species_OHE=SparseVector(3, {0: 1.0})),\n",
       " Row(SepalLengthCm=4.5, SepalWidthCm=2.3, PetalLengthCm=1.3, PetalWidthCm=0.3, Species=0, Species_OHE=SparseVector(3, {0: 1.0})),\n",
       " Row(SepalLengthCm=4.6, SepalWidthCm=3.1, PetalLengthCm=1.5, PetalWidthCm=0.2, Species=0, Species_OHE=SparseVector(3, {0: 1.0})),\n",
       " Row(SepalLengthCm=4.6, SepalWidthCm=3.2, PetalLengthCm=1.4, PetalWidthCm=0.2, Species=0, Species_OHE=SparseVector(3, {0: 1.0})),\n",
       " Row(SepalLengthCm=4.6, SepalWidthCm=3.4, PetalLengthCm=1.4, PetalWidthCm=0.3, Species=0, Species_OHE=SparseVector(3, {0: 1.0})),\n",
       " Row(SepalLengthCm=4.6, SepalWidthCm=3.6, PetalLengthCm=1.0, PetalWidthCm=0.2, Species=0, Species_OHE=SparseVector(3, {0: 1.0})),\n",
       " Row(SepalLengthCm=4.7, SepalWidthCm=3.2, PetalLengthCm=1.3, PetalWidthCm=0.2, Species=0, Species_OHE=SparseVector(3, {0: 1.0})),\n",
       " Row(SepalLengthCm=4.7, SepalWidthCm=3.2, PetalLengthCm=1.6, PetalWidthCm=0.2, Species=0, Species_OHE=SparseVector(3, {0: 1.0}))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 13:>                                                         (0 + 1) / 1]"
     ]
    }
   ],
   "source": [
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade58496",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
