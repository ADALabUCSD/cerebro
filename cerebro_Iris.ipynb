{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce6a85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/28 15:48:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/11/28 15:48:40 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-11-28 15:48:41, Running 1 Workers\n",
      "CEREBRO => Time: 2021-11-28 15:48:43, Preparing Data\n",
      "CEREBRO => Time: 2021-11-28 15:48:44, Num Partitions: 1\n",
      "CEREBRO => Time: 2021-11-28 15:48:44, Writing DataFrames\n",
      "CEREBRO => Time: 2021-11-28 15:48:44, Train Data Path: file:///Users/zijian/Desktop/ucsd/cse234/project/cerebro-system/experiments/intermediate_train_data\n",
      "CEREBRO => Time: 2021-11-28 15:48:44, Val Data Path: file:///Users/zijian/Desktop/ucsd/cse234/project/cerebro-system/experiments/intermediate_val_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 3:>                                                          (0 + 1) / 1]\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-11-28 15:48:45, Train Partitions: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-11-28 15:48:48, Val Partitions: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-11-28 15:48:51, Train Rows: 98\n",
      "CEREBRO => Time: 2021-11-28 15:48:51, Val Rows: 24\n",
      "CEREBRO => Time: 2021-11-28 15:48:51, Initializing Workers\n",
      "CEREBRO => Time: 2021-11-28 15:48:51, Initializing Data Loaders\n",
      "CEREBRO => Time: 2021-11-28 15:48:51, Launching Model Selection Workload\n",
      "[[-1, 1], [-1, 1], [-1, 1], [-1, 1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-28 15:48:51.515316: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-11-28 15:48:51.515512: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[Stage 10:>                                                         (0 + 1) / 1]2021-11-28 15:48:51.677717: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-11-28 15:48:51.781060: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-28 15:48:51.787369: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:196] None of the MLIR optimization passes are enabled (registered 0 passes)\n",
      "WARNING:tensorflow:From /Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:2561: calling DatasetV2.from_generator (from tensorflow.python.data.ops.dataset_ops) with output_types is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use output_signature instead\n",
      "WARNING:tensorflow:From /Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:2561: calling DatasetV2.from_generator (from tensorflow.python.data.ops.dataset_ops) with output_shapes is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use output_signature instead\n",
      "WARNING:tensorflow:Expected a shuffled dataset but input dataset `x` is not shuffled. Please invoke `shuffle()` on input dataset.\n",
      "2021-11-28 15:48:52.666298: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "Train on 7 steps\n",
      "7/7 [==============================] - 0s 1ms/step - batch: 3.0000 - size: 1.0000 - loss: 7.0057 - accuracy: 0.1429    \n",
      "CEREBRO => Time: 2021-11-28 15:48:53, Model: model_0_1638143331, Mode: TRAIN, Initialization Time: 0.8979220390319824, Training Time: 0.4216940402984619, Finalization Time: 0.12980103492736816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1, 1], [-1, 1], [-1, 1], [-1, 1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n",
      "2021-11-28 15:48:56.520398: W tensorflow/core/framework/op_kernel.cc:1751] Invalid argument: ValueError: callback pyfunc_2 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 238, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_2 is not found\n",
      "\n",
      "\n",
      "2021-11-28 15:48:56.520811: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Invalid argument: ValueError: callback pyfunc_2 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 238, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_2 is not found\n",
      "\n",
      "\n",
      "\t [[{{node PyFunc}}]]\n",
      "CEREBRO => Time: 2021-11-28 15:48:56, Model: model_0_1638143331, Mode: VALID, Initialization Time: 0.623093843460083, Training Time: 0.11697006225585938, Finalization Time: 0.08301806449890137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1, 1], [-1, 1], [-1, 1], [-1, 1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected a shuffled dataset but input dataset `x` is not shuffled. Please invoke `shuffle()` on input dataset.\n",
      "Train on 7 steps\n",
      "Epoch 2/2\n",
      "7/7 [==============================] - 0s 6ms/step - batch: 3.0000 - size: 1.0000 - loss: 1.1011 - accuracy: 0.5000    \n",
      "2021-11-28 15:49:00.669918: W tensorflow/core/framework/op_kernel.cc:1751] Invalid argument: ValueError: callback pyfunc_5 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 238, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_5 is not found\n",
      "\n",
      "\n",
      "2021-11-28 15:49:00.670197: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Invalid argument: ValueError: callback pyfunc_5 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/zijian/.pyenv/versions/nocerebro/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 238, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_5 is not found\n",
      "\n",
      "\n",
      "\t [[{{node PyFunc}}]]\n",
      "CEREBRO => Time: 2021-11-28 15:49:00, Model: model_0_1638143331, Mode: TRAIN, Initialization Time: 0.531959056854248, Training Time: 0.2724292278289795, Finalization Time: 0.11769795417785645\n"
     ]
    }
   ],
   "source": [
    "from cerebro.backend import SparkBackend\n",
    "from cerebro.keras import SparkEstimator\n",
    "\n",
    "# datas storage for intermediate data and model artifacts.\n",
    "from cerebro.storage import LocalStore, HDFSStore\n",
    "\n",
    "# Model selection/AutoML methods.\n",
    "from cerebro.tune import GridSearch, RandomSearch, TPESearch\n",
    "\n",
    "# Utility functions for specifying the search space.\n",
    "from cerebro.tune import hp_choice, hp_uniform, hp_quniform, hp_loguniform, hp_qloguniform\n",
    "\n",
    "import tensorflow as tf\n",
    "# tf.config.run_functions_eagerly(True)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Cerebro Iris\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "...\n",
    "\n",
    "backend = SparkBackend(spark_context=spark.sparkContext, num_workers=1)\n",
    "store = LocalStore(prefix_path='/Users/zijian/Desktop/ucsd/cse234/project/cerebro-system/experiments')\n",
    "\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "\n",
    "df = spark.read.csv(\"/Users/zijian/Desktop/ucsd/cse234/project/cerebro-system/Iris_clean.csv\", header=True, inferSchema=True)\n",
    "\n",
    "encoder = OneHotEncoderEstimator(dropLast=False)\n",
    "encoder.setInputCols([\"Species\"])\n",
    "encoder.setOutputCols([\"Species_OHE\"])\n",
    "\n",
    "encoder_model = encoder.fit(df)\n",
    "encoded = encoder_model.transform(df)\n",
    "\n",
    "feature_columns=['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']\n",
    "label_columns=['Species_OHE']\n",
    "\n",
    "# Initialize input DataFrames.\n",
    "# You can download sample dataset from https://apache.googlesource.com/spark/+/master/data/mllib/sample_libsvm_data.txt\n",
    "\n",
    "train_df, test_df = encoded.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Define estimator generating function.\n",
    "# Input: Dictionary containing parameter values\n",
    "# Output: SparkEstimator\n",
    "def estimator_gen_fn(params):\n",
    "#     inputs = [tf.keras.Input(shape=(1,)) for col in feature_columns]\n",
    "#     embeddings1 = [tf.keras.layers.Dense(16, activation=tf.nn.relu)(input) for input in inputs]\n",
    "#     embeddings2 = [tf.keras.layers.Dense(32, activation=tf.nn.relu)(input) for input in embeddings1]\n",
    "#     combined = tf.keras.layers.Concatenate()(embeddings2)\n",
    "#     output = tf.keras.layers.Dense(3, activation=tf.nn.softmax)(combined)\n",
    "#     model = tf.keras.Model(inputs, output)\n",
    "\n",
    "    inputs = [tf.keras.Input(shape=(1,)) for col in feature_columns]\n",
    "    concat = tf.keras.layers.Concatenate()(inputs)\n",
    "    output1 = tf.keras.layers.Dense(128, activation=tf.nn.relu)(concat)\n",
    "    output2 = tf.keras.layers.Dense(1024, activation=tf.nn.relu)(output1)\n",
    "    output = tf.keras.layers.Dense(3, activation=tf.nn.softmax)(output2)\n",
    "    model = tf.keras.Model(inputs, output)\n",
    "\n",
    "#     inputs = tf.keras.Input(shape=(4,))\n",
    "#     output1 = tf.keras.layers.Dense(16, activation=tf.nn.relu)(inputs)\n",
    "#     output2 = tf.keras.layers.Dense(32, activation=tf.nn.relu)(output1)\n",
    "#     output = tf.keras.layers.Dense(3, activation=tf.nn.softmax)(output2)\n",
    "#     model = tf.keras.Model(inputs, output)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=params['lr'])\n",
    "    loss = 'categorical_crossentropy'\n",
    "\n",
    "    estimator = SparkEstimator(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        loss=loss,\n",
    "        metrics=['accuracy'],\n",
    "        batch_size=params['batch_size'])\n",
    "\n",
    "    return estimator\n",
    "\n",
    "# Define dictionary containing the parameter search space.\n",
    "search_space = {\n",
    "    'lr': hp_choice([0.01]),\n",
    "    'batch_size': hp_choice([16])\n",
    "}\n",
    "\n",
    "# Instantiate TPE (Tree of Parzan Estimators a.k.a., HyperOpt) model selection object.\n",
    "model_selection = RandomSearch(\n",
    "    backend=backend, \n",
    "    store=store, \n",
    "    estimator_gen_fn=estimator_gen_fn, \n",
    "    search_space=search_space,\n",
    "    num_models=1, \n",
    "    num_epochs=20, \n",
    "    validation=0.2, \n",
    "    evaluation_metric='accuracy',\n",
    "    feature_columns=feature_columns,\n",
    "    label_columns=label_columns\n",
    ")\n",
    "\n",
    "# Perform model selection. Returns best model.\n",
    "model = model_selection.fit(train_df)\n",
    "\n",
    "# Inspect best model training history.\n",
    "model_history = model.get_history()\n",
    "\n",
    "# # Perform inference using the best model and Spark DataFrame.\n",
    "output_df = model.set_output_columns(['label_predicted']).transform(test_df)\n",
    "output_df.select('Species', 'label_predicted').show(n=10)\n",
    "\n",
    "# # Access all models.\n",
    "# all_models = model.get_all_models()\n",
    "# all_model_training_history = model.get_all_model_history()\n",
    "\n",
    "# # Convert the best model to Keras and perform inference using numpy data.\n",
    "# keras_model = model.keras()\n",
    "# pred = keras_model.predict([np.ones([1, 692], dtype=np.float32)])\n",
    "# # Save the keras checkpoint file.\n",
    "# keras_model.save(ckpt_path)\n",
    "\n",
    "# # Convert all the model to Keras.\n",
    "# all_models_keras = [m.keras() for m in all_models]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "759f5be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_0_1637592359': {'train_loss': [3.5598298708597818,\n",
       "   1.1557079950968425,\n",
       "   0.8183964689572653,\n",
       "   0.55611935009559,\n",
       "   0.5155076410155743,\n",
       "   0.48213007384523127,\n",
       "   0.35561879443654715,\n",
       "   0.4118161859223619,\n",
       "   0.25939790980676963,\n",
       "   0.38293483707820997,\n",
       "   0.1640534773662997,\n",
       "   0.2704889229886855,\n",
       "   0.1097771948358665,\n",
       "   0.28366075836432475,\n",
       "   0.12651007315920046,\n",
       "   0.44658533505329007,\n",
       "   1.1701089578370254,\n",
       "   0.3829073728993535,\n",
       "   0.39633384958142415,\n",
       "   0.41608361599598237],\n",
       "  'train_accuracy': [0.15625,\n",
       "   0.3333333432674408,\n",
       "   0.5729166865348816,\n",
       "   0.65625,\n",
       "   0.65625,\n",
       "   0.6770833134651184,\n",
       "   0.8645833134651184,\n",
       "   0.8020833134651184,\n",
       "   0.9375,\n",
       "   0.8125,\n",
       "   0.9479166865348816,\n",
       "   0.8958333134651184,\n",
       "   0.9583333134651184,\n",
       "   0.875,\n",
       "   0.9375,\n",
       "   0.84375,\n",
       "   0.5729166865348816,\n",
       "   0.6979166865348816,\n",
       "   0.8229166865348816,\n",
       "   0.8020833134651184],\n",
       "  'val_loss': [1.0147390365600586,\n",
       "   0.8646733462810516,\n",
       "   0.522622138261795,\n",
       "   0.5134254693984985,\n",
       "   0.4826420843601227,\n",
       "   0.4036994129419327,\n",
       "   0.34424158930778503,\n",
       "   0.26431381702423096,\n",
       "   0.19931229948997498,\n",
       "   0.20401012897491455,\n",
       "   0.14573021233081818,\n",
       "   0.1460253745317459,\n",
       "   0.1285715252161026,\n",
       "   0.17894812673330307,\n",
       "   0.49077482521533966,\n",
       "   1.0118185877799988,\n",
       "   0.5339368581771851,\n",
       "   0.4586724787950516,\n",
       "   0.5584477186203003,\n",
       "   0.35050706565380096],\n",
       "  'val_accuracy': [0.4375,\n",
       "   0.5,\n",
       "   0.75,\n",
       "   0.65625,\n",
       "   0.75,\n",
       "   0.8125,\n",
       "   0.9375,\n",
       "   0.9375,\n",
       "   1.0,\n",
       "   0.96875,\n",
       "   0.9375,\n",
       "   0.9375,\n",
       "   0.9375,\n",
       "   0.9375,\n",
       "   0.8125,\n",
       "   0.65625,\n",
       "   0.71875,\n",
       "   0.9375,\n",
       "   0.5625,\n",
       "   0.96875]}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b1a1ad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array(test_df.select(feature_columns).collect())\n",
    "y = np.array(test_df.select(label_columns).collect())\n",
    "x = [x[:,i,np.newaxis] for i in range(x.shape[1])]\n",
    "y = np.squeeze(y,1)\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a367b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2516 - accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-22 06:48:47.947852: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.25163865089416504, 1.0]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_model = model.get_best_model().getModel()\n",
    "keras_model.evaluate(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "547b5812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Species_OHE': {'spark_data_type': pyspark.sql.types.BinaryType,\n",
       "  'is_sparse_vector_only': False,\n",
       "  'shape': None,\n",
       "  'intermediate_format': 'nochange',\n",
       "  'max_size': None},\n",
       " 'SepalLengthCm': {'spark_data_type': pyspark.sql.types.DoubleType,\n",
       "  'is_sparse_vector_only': False,\n",
       "  'shape': None,\n",
       "  'intermediate_format': 'nochange',\n",
       "  'max_size': None},\n",
       " 'SepalWidthCm': {'spark_data_type': pyspark.sql.types.DoubleType,\n",
       "  'is_sparse_vector_only': False,\n",
       "  'shape': None,\n",
       "  'intermediate_format': 'nochange',\n",
       "  'max_size': None},\n",
       " 'PetalLengthCm': {'spark_data_type': pyspark.sql.types.DoubleType,\n",
       "  'is_sparse_vector_only': False,\n",
       "  'shape': None,\n",
       "  'intermediate_format': 'nochange',\n",
       "  'max_size': None},\n",
       " 'PetalWidthCm': {'spark_data_type': pyspark.sql.types.DoubleType,\n",
       "  'is_sparse_vector_only': False,\n",
       "  'shape': None,\n",
       "  'intermediate_format': 'nochange',\n",
       "  'max_size': None}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_best_model()._get_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf949ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-11-22 06:48:48, Running 1 Workers\n",
      "CEREBRO => Time: 2021-11-22 06:48:48, Num Partitions: 1\n",
      "CEREBRO => Time: 2021-11-22 06:48:48, Writing DataFrames\n",
      "CEREBRO => Time: 2021-11-22 06:48:48, Train Data Path: file:///Users/zijian/Desktop/ucsd/cse234/project/cerebro-system/experiments/intermediate_train_data\n",
      "CEREBRO => Time: 2021-11-22 06:48:48, Val Data Path: file:///Users/zijian/Desktop/ucsd/cse234/project/cerebro-system/experiments/intermediate_val_data\n",
      "CEREBRO => Time: 2021-11-22 06:48:49, Train Partitions: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-11-22 06:48:52, Val Partitions: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-11-22 06:48:54, Train Rows: 94\n",
      "CEREBRO => Time: 2021-11-22 06:48:54, Val Rows: 23\n"
     ]
    }
   ],
   "source": [
    "from cerebro.backend import SparkBackend\n",
    "from cerebro.keras import SparkEstimator\n",
    "\n",
    "# datas storage for intermediate data and model artifacts.\n",
    "from cerebro.storage import LocalStore, HDFSStore\n",
    "\n",
    "# Model selection/AutoML methods.\n",
    "from cerebro.tune import GridSearch, RandomSearch, TPESearch\n",
    "\n",
    "# Utility functions for specifying the search space.\n",
    "from cerebro.tune import hp_choice, hp_uniform, hp_quniform, hp_loguniform, hp_qloguniform\n",
    "\n",
    "import tensorflow as tf\n",
    "# tf.config.run_functions_eagerly(True)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Cerebro Iris\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "...\n",
    "\n",
    "backend = SparkBackend(spark_context=spark.sparkContext, num_workers=1)\n",
    "store = LocalStore(prefix_path='/Users/zijian/Desktop/ucsd/cse234/project/cerebro-system/experiments')\n",
    "\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "\n",
    "df = spark.read.csv(\"/Users/zijian/Desktop/ucsd/cse234/project/cerebro-system/Iris_clean.csv\", header=True, inferSchema=True)\n",
    "\n",
    "encoder = OneHotEncoderEstimator(dropLast=False)\n",
    "encoder.setInputCols([\"Species\"])\n",
    "encoder.setOutputCols([\"Species_OHE\"])\n",
    "\n",
    "encoder_model = encoder.fit(df)\n",
    "encoded = encoder_model.transform(df)\n",
    "\n",
    "feature_columns=['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']\n",
    "label_columns=['Species_OHE']\n",
    "\n",
    "# Initialize input DataFrames.\n",
    "# You can download sample dataset from https://apache.googlesource.com/spark/+/master/data/mllib/sample_libsvm_data.txt\n",
    "\n",
    "train_df, test_df = encoded.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Define estimator generating function.\n",
    "# Input: Dictionary containing parameter values\n",
    "# Output: SparkEstimator\n",
    "def estimator_gen_fn(params):\n",
    "    inputs = [tf.keras.Input(shape=(1,)) for col in feature_columns]\n",
    "    embeddings = [tf.keras.layers.Dense(16, activation=tf.nn.relu)(input) for input in inputs]\n",
    "    combined = tf.keras.layers.Concatenate()(embeddings)\n",
    "    output = tf.keras.layers.Dense(3, activation=tf.nn.softmax)(combined)\n",
    "    model = tf.keras.Model(inputs, output)\n",
    "\n",
    "#     inputs = tf.keras.Input(shape=(4,))\n",
    "#     output1 = tf.keras.layers.Dense(16, activation=tf.nn.relu)(inputs)\n",
    "#     output2 = tf.keras.layers.Dense(32, activation=tf.nn.relu)(output1)\n",
    "#     output = tf.keras.layers.Dense(3, activation=tf.nn.softmax)(output2)\n",
    "#     model = tf.keras.Model(inputs, output)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=params['lr'])\n",
    "    loss = 'categorical_crossentropy'\n",
    "\n",
    "    estimator = SparkEstimator(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        loss=loss,\n",
    "        metrics=['accuracy'],\n",
    "        batch_size=params['batch_size'])\n",
    "\n",
    "    return estimator\n",
    "\n",
    "# Define dictionary containing the parameter search space.\n",
    "search_space = {\n",
    "    'lr': hp_choice([0.01, 0.001, 0.0001]),\n",
    "    'batch_size': hp_quniform(16, 64, 16)\n",
    "}\n",
    "\n",
    "# Instantiate TPE (Tree of Parzan Estimators a.k.a., HyperOpt) model selection object.\n",
    "model_selection = TPESearch(\n",
    "    backend=backend, \n",
    "    store=store, \n",
    "    estimator_gen_fn=estimator_gen_fn, \n",
    "    search_space=search_space,\n",
    "    num_models=1, \n",
    "    num_epochs=10, \n",
    "    validation=0.25, \n",
    "    evaluation_metric='loss',\n",
    "    feature_columns=feature_columns,\n",
    "    label_columns=label_columns\n",
    ")\n",
    "\n",
    "_, _, metadata, _ = model_selection.backend.prepare_data(model_selection.store, train_df, model_selection.validation, label_columns=model_selection.label_cols, feature_columns=model_selection.feature_cols)\n",
    "\n",
    "model_selection.backend.initialize_workers()\n",
    "\n",
    "model_selection.backend.initialize_data_loaders(model_selection.store, None, model_selection.feature_cols + model_selection.label_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cb5d93b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Species_OHE': {'spark_data_type': pyspark.sql.types.BinaryType,\n",
       "  'is_sparse_vector_only': False,\n",
       "  'shape': None,\n",
       "  'intermediate_format': 'nochange',\n",
       "  'max_size': None},\n",
       " 'SepalLengthCm': {'spark_data_type': pyspark.sql.types.DoubleType,\n",
       "  'is_sparse_vector_only': False,\n",
       "  'shape': None,\n",
       "  'intermediate_format': 'nochange',\n",
       "  'max_size': None},\n",
       " 'SepalWidthCm': {'spark_data_type': pyspark.sql.types.DoubleType,\n",
       "  'is_sparse_vector_only': False,\n",
       "  'shape': None,\n",
       "  'intermediate_format': 'nochange',\n",
       "  'max_size': None},\n",
       " 'PetalLengthCm': {'spark_data_type': pyspark.sql.types.DoubleType,\n",
       "  'is_sparse_vector_only': False,\n",
       "  'shape': None,\n",
       "  'intermediate_format': 'nochange',\n",
       "  'max_size': None},\n",
       " 'PetalWidthCm': {'spark_data_type': pyspark.sql.types.DoubleType,\n",
       "  'is_sparse_vector_only': False,\n",
       "  'shape': None,\n",
       "  'intermediate_format': 'nochange',\n",
       "  'max_size': None}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e6ba32f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SepalLengthCm': {'spark_data_type': pyspark.sql.types.DoubleType,\n",
       "  'is_sparse_vector_only': False,\n",
       "  'shape': 1,\n",
       "  'intermediate_format': 'nochange',\n",
       "  'max_size': 1},\n",
       " 'SepalWidthCm': {'spark_data_type': pyspark.sql.types.DoubleType,\n",
       "  'is_sparse_vector_only': False,\n",
       "  'shape': 1,\n",
       "  'intermediate_format': 'nochange',\n",
       "  'max_size': 1},\n",
       " 'PetalLengthCm': {'spark_data_type': pyspark.sql.types.DoubleType,\n",
       "  'is_sparse_vector_only': False,\n",
       "  'shape': 1,\n",
       "  'intermediate_format': 'nochange',\n",
       "  'max_size': 1},\n",
       " 'PetalWidthCm': {'spark_data_type': pyspark.sql.types.DoubleType,\n",
       "  'is_sparse_vector_only': False,\n",
       "  'shape': 1,\n",
       "  'intermediate_format': 'nochange',\n",
       "  'max_size': 1},\n",
       " 'Species': {'spark_data_type': pyspark.sql.types.IntegerType,\n",
       "  'is_sparse_vector_only': False,\n",
       "  'shape': 1,\n",
       "  'intermediate_format': 'nochange',\n",
       "  'max_size': 1},\n",
       " 'Species_OHE': {'spark_data_type': pyspark.ml.linalg.SparseVector,\n",
       "  'is_sparse_vector_only': True,\n",
       "  'shape': 3,\n",
       "  'intermediate_format': 'custom_sparse_format',\n",
       "  'max_size': 1}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cerebro.backend.spark.util import _get_metadata\n",
    "\n",
    "_get_metadata(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f35750b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.0>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=0.2>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.6>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.6>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 0., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.1>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=0.1>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.3>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.0>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 0., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.2>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=0.2>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.8>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.0>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 0., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.3>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=0.2>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.4>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.0>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 0., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.3>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=0.2>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.5>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.5>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 0., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.3>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=0.3>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.5>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.3>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 0., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.3>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=0.4>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.4>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.9>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 0., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.4>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=0.1>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.8>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.0>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 0., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.4>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=0.2>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.4>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.9>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 0., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.4>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=0.2>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.6>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.2>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 0., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.4>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=0.2>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.9>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.0>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 0., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.4>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=0.2>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.0>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.3>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 0., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.4>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=0.2>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.0>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.6>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 0., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.4>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=0.2>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.5>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.2>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 0., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.4>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=0.3>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.8>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.0>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 0., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.4>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=0.3>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.1>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.5>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 0., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.5>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=0.1>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.9>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.1>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 0., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.5>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=0.1>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.9>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.1>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 0., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.5>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=0.1>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.2>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.1>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 0., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.5>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=0.2>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.2>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.5>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 0., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.5>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=0.2>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.4>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.7>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 0., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.5>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=0.3>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.1>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.8>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 0., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.5>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=0.4>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.7>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.4>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 0., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.6>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=0.2>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.7>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.2>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 0., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.6>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=0.2>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.8>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.4>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 0., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.6>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=0.2>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.0>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.0>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 0., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.6>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=0.2>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.1>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.8>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 0., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.7>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=0.2>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.4>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.4>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 0., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.7>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=0.3>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.7>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.8>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 0., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.7>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=0.4>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.4>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.9>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 0., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.7>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=0.5>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.1>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.3>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 0., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.9>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=0.4>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.1>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.8>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 0., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.3>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.0>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.0>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.3>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.5>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.0>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.7>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.6>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.8>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.1>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.5>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.4>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.9>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.1>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.6>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.5>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.9>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.2>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.8>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.7>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.0>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.0>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.0>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.2>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.0>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.3>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.5>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.3>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.0>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.3>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.5>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.5>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.0>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.3>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.1>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.8>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.1>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.3>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.6>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.0>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.1>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.3>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.7>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.8>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.2>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.2>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.7>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.0>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.2>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.3>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.6>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.7>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.2>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.3>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.7>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.9>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.2>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.5>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.9>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.0>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.3>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.3>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.4>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.9>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.4>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.2>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.5>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.6>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.4>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.3>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.3>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.3>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.4>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.4>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.6>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.0>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.5>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.3>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.7>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.8>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.5>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.5>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.4>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.0>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.5>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.5>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.0>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.9>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.5>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.5>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.4>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.2>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.5>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.6>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.0>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.4>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.5>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.7>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.9>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.5>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 0., 1.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.6>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.3>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.6>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.9>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.6>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.4>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.1>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.0>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.7>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.2>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.1>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.8>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.7>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.4>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.1>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.9>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.7>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.4>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=7.0>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.2>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.7>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.5>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.7>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.1>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.7>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.6>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.3>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.3>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.8>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.4>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.8>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.8>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.8>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.8>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.9>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.2>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.8>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.8>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.2>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.8>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 0., 1.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.9>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.5>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.3>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.5>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=4.9>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.8>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.3>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.7>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 0., 1.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.0>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.9>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.3>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.5>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 0., 1.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.0>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.0>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.7>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.5>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 0., 1.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.1>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.6>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.0>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.7>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 1., 0.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.1>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.8>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.9>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.0>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 0., 1.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.1>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.9>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.8>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.7>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 0., 1.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.1>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.0>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.5>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.2>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 0., 1.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.1>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.3>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.9>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.1>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 0., 1.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.2>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.0>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.5>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.0>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 0., 1.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.2>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.3>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.7>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.0>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 0., 1.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.3>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.3>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.4>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.2>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 0., 1.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.4>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.1>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.9>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.1>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 0., 1.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.6>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.8>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.3>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.9>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 0., 1.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.6>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.4>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.3>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.4>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 0., 1.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.7>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.3>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.9>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.2>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 0., 1.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.7>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.5>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.7>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.3>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 0., 1.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.8>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.6>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=7.2>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.0>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 0., 1.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.8>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.2>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.5>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.0>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 0., 1.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=5.9>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.1>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=7.1>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.0>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 0., 1.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.0>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.8>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=7.2>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.2>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 0., 1.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.1>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.9>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=7.4>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.8>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 0., 1.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.1>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.3>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=7.7>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.0>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 0., 1.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.3>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=1.8>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=7.3>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.9>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 0., 1.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.4>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.0>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=7.9>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.8>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 0., 1.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.6>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.1>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=7.6>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=3.0>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 0., 1.])>)\n",
      "petastorm_schema_view(PetalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=6.7>, PetalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.0>, SepalLengthCm=<tf.Tensor: shape=(), dtype=float64, numpy=7.7>, SepalWidthCm=<tf.Tensor: shape=(), dtype=float64, numpy=2.8>, Species_OHE=<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 0., 1.])>)\n"
     ]
    }
   ],
   "source": [
    "from petastorm import make_reader\n",
    "\n",
    "from petastorm.tf_utils import make_petastorm_dataset\n",
    "\n",
    "with make_reader('file:///Users/zijian/Desktop/ucsd/cse234/project/cerebro-system/experiments/intermediate_train_data') as reader:\n",
    "    dataset = make_petastorm_dataset(reader)\n",
    "    for ele in dataset:\n",
    "        print(ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12af8367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(SepalLengthCm=4.3, SepalWidthCm=3.0, PetalLengthCm=1.1, PetalWidthCm=0.1, Species=0, Species_OHE=SparseVector(3, {0: 1.0})),\n",
       " Row(SepalLengthCm=4.4, SepalWidthCm=2.9, PetalLengthCm=1.4, PetalWidthCm=0.2, Species=0, Species_OHE=SparseVector(3, {0: 1.0})),\n",
       " Row(SepalLengthCm=4.4, SepalWidthCm=3.0, PetalLengthCm=1.3, PetalWidthCm=0.2, Species=0, Species_OHE=SparseVector(3, {0: 1.0})),\n",
       " Row(SepalLengthCm=4.4, SepalWidthCm=3.2, PetalLengthCm=1.3, PetalWidthCm=0.2, Species=0, Species_OHE=SparseVector(3, {0: 1.0})),\n",
       " Row(SepalLengthCm=4.5, SepalWidthCm=2.3, PetalLengthCm=1.3, PetalWidthCm=0.3, Species=0, Species_OHE=SparseVector(3, {0: 1.0})),\n",
       " Row(SepalLengthCm=4.6, SepalWidthCm=3.2, PetalLengthCm=1.4, PetalWidthCm=0.2, Species=0, Species_OHE=SparseVector(3, {0: 1.0})),\n",
       " Row(SepalLengthCm=4.6, SepalWidthCm=3.6, PetalLengthCm=1.0, PetalWidthCm=0.2, Species=0, Species_OHE=SparseVector(3, {0: 1.0})),\n",
       " Row(SepalLengthCm=4.7, SepalWidthCm=3.2, PetalLengthCm=1.6, PetalWidthCm=0.2, Species=0, Species_OHE=SparseVector(3, {0: 1.0})),\n",
       " Row(SepalLengthCm=4.8, SepalWidthCm=3.0, PetalLengthCm=1.4, PetalWidthCm=0.1, Species=0, Species_OHE=SparseVector(3, {0: 1.0})),\n",
       " Row(SepalLengthCm=4.8, SepalWidthCm=3.0, PetalLengthCm=1.4, PetalWidthCm=0.3, Species=0, Species_OHE=SparseVector(3, {0: 1.0}))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:>                                                         (0 + 1) / 1]"
     ]
    }
   ],
   "source": [
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ededb40b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
