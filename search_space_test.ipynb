{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "223daae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from keras_tuner import HyperParameters\n",
    "\n",
    "import autokeras as ak\n",
    "\n",
    "from cerebro.nas.hphpmodel import HyperHyperModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b493757",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_node = ak.StructuredDataInput()\n",
    "output_node = ak.StructuredDataBlock(categorical_encoding=True)(input_node)\n",
    "output_node = ak.ClassificationHead()(output_node)\n",
    "am = HyperHyperModel(\n",
    "    inputs=input_node, outputs=output_node, overwrite=True, max_trials=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b4dfdc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/19 22:31:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-11-19 22:31:02, Running 1 Workers\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Build the SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "   .appName(\"Linear Regression Model\") \\\n",
    "   .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "from cerebro.backend import SparkBackend\n",
    "from cerebro.storage import LocalStore\n",
    "\n",
    "backend = SparkBackend(spark_context=sc, num_workers=1)\n",
    "store = LocalStore(prefix_path='/Users/zijian/Desktop/ucsd/cse234/project/cerebro-system/experiments')\n",
    "\n",
    "am.resource_bind(\n",
    "    backend=backend, \n",
    "    store=store,\n",
    "    feature_columns=['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm'],\n",
    "    label_columns=['Species']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee1c1051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(SepalLengthCm='5.1', SepalWidthCm='3.5', PetalLengthCm='1.4', PetalWidthCm='0.2', Species='0'),\n",
       " Row(SepalLengthCm='4.9', SepalWidthCm='3.0', PetalLengthCm='1.4', PetalWidthCm='0.2', Species='0'),\n",
       " Row(SepalLengthCm='4.7', SepalWidthCm='3.2', PetalLengthCm='1.3', PetalWidthCm='0.2', Species='0'),\n",
       " Row(SepalLengthCm='4.6', SepalWidthCm='3.1', PetalLengthCm='1.5', PetalWidthCm='0.2', Species='0'),\n",
       " Row(SepalLengthCm='5.0', SepalWidthCm='3.6', PetalLengthCm='1.4', PetalWidthCm='0.2', Species='0'),\n",
       " Row(SepalLengthCm='5.4', SepalWidthCm='3.9', PetalLengthCm='1.7', PetalWidthCm='0.4', Species='0'),\n",
       " Row(SepalLengthCm='4.6', SepalWidthCm='3.4', PetalLengthCm='1.4', PetalWidthCm='0.3', Species='0'),\n",
       " Row(SepalLengthCm='5.0', SepalWidthCm='3.4', PetalLengthCm='1.5', PetalWidthCm='0.2', Species='0'),\n",
       " Row(SepalLengthCm='4.4', SepalWidthCm='2.9', PetalLengthCm='1.4', PetalWidthCm='0.2', Species='0'),\n",
       " Row(SepalLengthCm='4.9', SepalWidthCm='3.1', PetalLengthCm='1.5', PetalWidthCm='0.1', Species='0')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.csv(\"/Users/zijian/Desktop/ucsd/cse234/project/cerebro-system/Iris_clean.csv\", header=True)\n",
    "df = df.toPandas()\n",
    "\n",
    "train=df.sample(frac=0.8,random_state=200) #random state is a seed value\n",
    "test=df.drop(train.index)\n",
    "\n",
    "df.head(10)\n",
    "# train_df, test_df = df.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a330d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 4)\n",
      "(120, 1)\n"
     ]
    }
   ],
   "source": [
    "train_np = np.array(train)\n",
    "x_train = train_np[:,:-1]\n",
    "y_train = train_np[:,-1,np.newaxis]\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b45bb36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project ./test/oracle.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-19 22:31:05.973877: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-11-19 22:31:05.974195: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 10\n",
      "learning_rate (Choice)\n",
      "{'default': 0.1, 'conditions': [], 'values': [0.1, 0.01], 'ordered': True}\n",
      "batch_size (Choice)\n",
      "{'default': 32, 'conditions': [], 'values': [32, 64, 128], 'ordered': True}\n",
      "structured_data_block_1/normalize (Boolean)\n",
      "{'default': False, 'conditions': []}\n",
      "structured_data_block_1/dense_block_1/use_batchnorm (Boolean)\n",
      "{'default': False, 'conditions': []}\n",
      "structured_data_block_1/dense_block_1/num_layers (Choice)\n",
      "{'default': 2, 'conditions': [], 'values': [1, 2, 3], 'ordered': True}\n",
      "structured_data_block_1/dense_block_1/units_0 (Choice)\n",
      "{'default': 32, 'conditions': [], 'values': [16, 32, 64, 128, 256, 512, 1024], 'ordered': True}\n",
      "structured_data_block_1/dense_block_1/dropout (Choice)\n",
      "{'default': 0.0, 'conditions': [], 'values': [0.0, 0.25, 0.5], 'ordered': True}\n",
      "structured_data_block_1/dense_block_1/units_1 (Choice)\n",
      "{'default': 32, 'conditions': [], 'values': [16, 32, 64, 128, 256, 512, 1024], 'ordered': True}\n",
      "classification_head_1/dropout (Choice)\n",
      "{'default': 0, 'conditions': [], 'values': [0.0, 0.25, 0.5], 'ordered': True}\n",
      "optimizer (Choice)\n",
      "{'default': 'adam', 'conditions': [], 'values': ['adam', 'sgd', 'adam_weight_decay'], 'ordered': False}\n"
     ]
    }
   ],
   "source": [
    "cuz_hps = HyperParameters()\n",
    "cuz_hps.Choice('learning_rate', values=[0.1,0.01])\n",
    "cuz_hps.Choice('batch_size', values=[32,64,128])\n",
    "\n",
    "am.tuner_bind(tuner=\"randomsearch\", hyperparameters=cuz_hps)\n",
    "am.test_tuner_space(x=x_train, y=y_train)\n",
    "\n",
    "am.tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cddd318c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.1, 'batch_size': 32, 'structured_data_block_1/normalize': False, 'structured_data_block_1/dense_block_1/use_batchnorm': False, 'structured_data_block_1/dense_block_1/num_layers': 3, 'structured_data_block_1/dense_block_1/units_0': 64, 'structured_data_block_1/dense_block_1/dropout': 0.25, 'structured_data_block_1/dense_block_1/units_1': 1024, 'classification_head_1/dropout': 0.5, 'optimizer': 'adam'}\n",
      "{'learning_rate': 0.1, 'batch_size': 64, 'structured_data_block_1/normalize': False, 'structured_data_block_1/dense_block_1/use_batchnorm': True, 'structured_data_block_1/dense_block_1/num_layers': 1, 'structured_data_block_1/dense_block_1/units_0': 256, 'structured_data_block_1/dense_block_1/dropout': 0.5, 'structured_data_block_1/dense_block_1/units_1': 512, 'classification_head_1/dropout': 0.0, 'optimizer': 'sgd'}\n"
     ]
    }
   ],
   "source": [
    "tuner = am.tuner\n",
    "\n",
    "trials = tuner.oracle.create_trials(2)\n",
    "for trial in trials:\n",
    "    print(trial.hyperparameters.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7175a24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.01,\n",
       " 'batch_size': 128,\n",
       " 'structured_data_block_1/normalize': False,\n",
       " 'structured_data_block_1/dense_block_1/use_batchnorm': True,\n",
       " 'structured_data_block_1/dense_block_1/num_layers': 2,\n",
       " 'structured_data_block_1/dense_block_1/units_0': 512,\n",
       " 'structured_data_block_1/dense_block_1/dropout': 0.5,\n",
       " 'structured_data_block_1/dense_block_1/units_1': 128,\n",
       " 'classification_head_1/dropout': 0.0,\n",
       " 'optimizer': 'adam_weight_decay'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kt_trial = tuner.oracle.create_trial(tuner.tuner_id)\n",
    "kt_trial.hyperparameters.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "043cf4f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'KerasHyperModel' object has no attribute '_get_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/dg/4jghvsmd1dl77gj1fqmzzf_80000gn/T/ipykernel_1071/2155497442.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'KerasHyperModel' object has no attribute '_get_loss'"
     ]
    }
   ],
   "source": [
    "tuner.hypermodel._get_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4189895c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
