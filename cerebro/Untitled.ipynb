{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45d6e556",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_tuner.engine.hypermodel import HyperModel\n",
    "\n",
    "from autokeras.engine.tuner import AutoTuner\n",
    "from cerebro.keras import SparkEstimator\n",
    "from cerebro.tune.base import ModelSelection\n",
    "\n",
    "\n",
    "class SparkTuner(AutoTuner):\n",
    "    \"\"\"\n",
    "    SparkTuner inherits AutoTuner to tune the preprocessor blocks. Also it over-writes run_trial method to using cerebro as the underling training system\n",
    "\n",
    "    [param] - [backend, store, validation, evaluation_metric, label_column, feature_column, verbose]: For construction of modelsection object\n",
    "\n",
    "    param - oracle: Tuning kernel\n",
    "    param - hypermodel: Hypermodel which implements build method, hypermodel.build(hp) will give a keras model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            oracle,\n",
    "            hypermodel: HyperModel,\n",
    "            backend, store, validation, evaluation_metric, label_columns, feature_columns, verbose,\n",
    "            **kwargs):\n",
    "\n",
    "        \"\"\"\n",
    "        Default estimator_generation_function used to initialize model_selection class, all parameters are extracted from params\n",
    "        \"\"\"\n",
    "        def est_gen_fun(params):\n",
    "            model = params['model']\n",
    "            opt = params['optimizer']\n",
    "            loss = params['loss']\n",
    "            metrics = params['metrics']\n",
    "            bs = params['batch_size']\n",
    "            estimator = SparkEstimator(\n",
    "                model=model,\n",
    "                optimizer=opt,\n",
    "                loss=loss,\n",
    "                metrics=metrics,\n",
    "                batch_size=bs\n",
    "            )\n",
    "            return estimator\n",
    "\n",
    "        \"\"\"\n",
    "        We temporarily keep the entire ModelSelection class here for compatibility and potential usage of its methods\n",
    "        It encapsulates backend engine which shall be used in the training process\n",
    "        \"\"\"\n",
    "        self.model_section = ModelSelection(\n",
    "            backend,\n",
    "            store,\n",
    "            validation,\n",
    "            est_gen_fun,\n",
    "            evaluation_metric,\n",
    "            label_columns,\n",
    "            feature_columns,\n",
    "            verbose\n",
    "        )\n",
    "\n",
    "        super().__init__(oracle, hypermodel, **kwargs)\n",
    "\n",
    "    \"\"\"\n",
    "    Over-write this function to train one epoch using cerebro\n",
    "\n",
    "    kwargs[\"x\"] is a tf.data.dataset containing train_x and train_y\n",
    "    \"\"\"\n",
    "    def _build_and_fit_model(self, trial, *args, **kwargs):\n",
    "        (\n",
    "            pipeline,\n",
    "            kwargs[\"x\"],\n",
    "            kwargs[\"validation_data\"],\n",
    "        ) = self._prepare_model_build(trial.hyperparameters, **kwargs)\n",
    "        pipeline.save(self._pipeline_path(trial.trial_id))\n",
    "\n",
    "        model = self.hypermodel.build(trial.hyperparameters)\n",
    "        self.adapt(model, kwargs[\"x\"])\n",
    "        params = {\n",
    "            'model': model,\n",
    "            'optimizer': model.optimizer, # keras opt not str\n",
    "            'loss': self.hypermodel._get_loss(), # not sure\n",
    "            'metrics': self.hypermodel._get_metrics(),\n",
    "            'bs': self.hypermodel.batch_size\n",
    "        }\n",
    "        _, history = self.spark_fit(\n",
    "            params, **kwargs\n",
    "        )\n",
    "        return history\n",
    "\n",
    "    \"\"\"\n",
    "    Train a generated model with params as hyperparameter\n",
    "    The model is wrapped with spark estimator and is trained for one epoch.\n",
    "\n",
    "    params: normal training hyperparameter to construct the estimator\n",
    "    kwargs['x']: tf.data.dataset.zip(train_x, train_y)\n",
    "    kwargs['validation_data']: \n",
    "    \"\"\"\n",
    "    def spark_fit(self, params, **kwargs):\n",
    "        \n",
    "        est = self.ms._estimator_gen_fn_wrapper(params)\n",
    "        #TODO Log to tensorboard\n",
    "        ms = self.model_section\n",
    "        epoch_rel = ms.backend.train_for_one_epoch(est, ms.store, ms.feature_cols, ms.label_cols)\n",
    "        hist = 0\n",
    "        for k in epoch_rel:\n",
    "            hist = hist + epoch_rel[k]\n",
    "        return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d205733c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner\n",
    "\n",
    "class Hyperband(keras_tuner.Hyperband, SparkTuner):\n",
    "    \"\"\"KerasTuner Hyperband with preprocessing layer tuning.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, max_epochs: int = 100, max_trials: int = 100, *args, **kwargs\n",
    "    ):\n",
    "        super().__init__(max_epochs=max_epochs, *args, **kwargs)\n",
    "        self.oracle.max_trials = max_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b05c8f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autokeras as ak\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from typing import Optional\n",
    "from typing import Type\n",
    "from typing import Union\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.util import nest\n",
    "\n",
    "from autokeras import blocks\n",
    "from autokeras import graph as graph_module\n",
    "from autokeras import pipeline\n",
    "from autokeras import tuners\n",
    "from autokeras.engine import head as head_module\n",
    "from autokeras.engine import node as node_module\n",
    "from autokeras.engine import tuner\n",
    "from autokeras.nodes import Input\n",
    "from autokeras.utils import data_utils\n",
    "from autokeras.utils import utils\n",
    "\n",
    "NAS_TUNERS = {\n",
    "    \"hyperband\": Hyperband\n",
    "}\n",
    "\n",
    "def get_tuner_class(tuner):\n",
    "    if isinstance(tuner, str) and tuner in NAS_TUNERS:\n",
    "        return NAS_TUNERS.get(tuner)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            'Expected the tuner argument to be one of \"greedy\", '\n",
    "            '\"random\", \"hyperband\", or \"bayesian\", '\n",
    "            \"but got {tuner}\".format(tuner=tuner)\n",
    "        )\n",
    "\n",
    "class HyperHyperModel(object):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        inputs: Union[Input, List[Input]],\n",
    "        outputs: Union[head_module.Head, node_module.Node, list],\n",
    "        overwrite: bool = False,\n",
    "        seed: Optional[int] = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.inputs = nest.flatten(inputs)\n",
    "        self.outputs = nest.flatten(outputs)\n",
    "        self.seed = seed\n",
    "        if seed:\n",
    "            np.random.seed(seed)\n",
    "            tf.random.set_seed(seed)\n",
    "        # TODO: Support passing a tuner instance.\n",
    "        # Initialize the hyper_graph.\n",
    "        self.graph = self._build_graph()\n",
    "        self.overwrite = overwrite\n",
    "        self._heads = [output_node.in_blocks[0] for output_node in self.outputs]\n",
    "\n",
    "    def tuner_bind(self, \n",
    "        backend,\n",
    "        store,\n",
    "        tuner: Union[str, Type[SparkTuner]] = \"hyperband\",\n",
    "        validation=0.2, \n",
    "        evaluation_metric='loss', \n",
    "        label_columns='labels', \n",
    "        feature_columns='features', \n",
    "        verbose=1,\n",
    "        project_name: str = \"test\",\n",
    "        max_trials: int = 100,\n",
    "        directory: Union[str, Path, None] = None,\n",
    "        objective: str = \"val_loss\",\n",
    "        overwrite: bool = False,\n",
    "        max_model_size: Optional[int] = None,\n",
    "        **kwargs):\n",
    "        if isinstance(tuner, str):\n",
    "            tuner = get_tuner_class(tuner)\n",
    "        self.tuner = tuner(\n",
    "            backend=backend,\n",
    "            store=store,\n",
    "            validation=validation,\n",
    "            evaluation_metric=evaluation_metric,\n",
    "            label_columns=label_columns,\n",
    "            feature_columns=feature_columns,\n",
    "            verbose=verbose,\n",
    "            hypermodel=self.graph,\n",
    "            overwrite=overwrite,\n",
    "            objective=objective,\n",
    "            max_trials=max_trials,\n",
    "            directory=directory,\n",
    "            seed=self.seed,\n",
    "            project_name=project_name,\n",
    "            max_model_size=max_model_size,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "\n",
    "    @property\n",
    "    def objective(self):\n",
    "        return self.tuner.objective\n",
    "\n",
    "    @property\n",
    "    def max_trials(self):\n",
    "        return self.tuner.max_trials\n",
    "\n",
    "    @property\n",
    "    def directory(self):\n",
    "        return self.tuner.directory\n",
    "\n",
    "    @property\n",
    "    def project_name(self):\n",
    "        return self.tuner.project_name\n",
    "\n",
    "    def _assemble(self):\n",
    "        \"\"\"Assemble the Blocks based on the input output nodes.\"\"\"\n",
    "        inputs = nest.flatten(self.inputs)\n",
    "        outputs = nest.flatten(self.outputs)\n",
    "\n",
    "        middle_nodes = [input_node.get_block()(input_node) for input_node in inputs]\n",
    "\n",
    "        # Merge the middle nodes.\n",
    "        if len(middle_nodes) > 1:\n",
    "            output_node = blocks.Merge()(middle_nodes)\n",
    "        else:\n",
    "            output_node = middle_nodes[0]\n",
    "\n",
    "        outputs = nest.flatten(\n",
    "            [output_blocks(output_node) for output_blocks in outputs]\n",
    "        )\n",
    "        return graph_module.Graph(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    def _build_graph(self):\n",
    "        # Using functional API.\n",
    "        if all([isinstance(output, node_module.Node) for output in self.outputs]):\n",
    "            graph = graph_module.Graph(inputs=self.inputs, outputs=self.outputs)\n",
    "        # Using input/output API.\n",
    "        elif all([isinstance(output, head_module.Head) for output in self.outputs]):\n",
    "            # Clear session to reset get_uid(). The names of the blocks will\n",
    "            # start to count from 1 for new blocks in a new AutoModel afterwards.\n",
    "            tf.keras.backend.clear_session()\n",
    "            graph = self._assemble()\n",
    "            self.outputs = graph.outputs\n",
    "            tf.keras.backend.clear_session()\n",
    "\n",
    "        return graph\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        x=None,\n",
    "        y=None,\n",
    "        batch_size=32,\n",
    "        epochs=None,\n",
    "        callbacks=None,\n",
    "        validation_split=0.2,\n",
    "        validation_data=None,\n",
    "        verbose=1,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"Search for the best model and hyperparameters for the AutoModel.\n",
    "\n",
    "        It will search for the best model based on the performances on\n",
    "        validation data.\n",
    "\n",
    "        # Arguments\n",
    "            x: numpy.ndarray or tensorflow.Dataset. Training data x.\n",
    "            y: numpy.ndarray or tensorflow.Dataset. Training data y.\n",
    "            batch_size: Int. Number of samples per gradient update. Defaults to 32.\n",
    "            epochs: Int. The number of epochs to train each model during the search.\n",
    "                If unspecified, by default we train for a maximum of 1000 epochs,\n",
    "                but we stop training if the validation loss stops improving for 10\n",
    "                epochs (unless you specified an EarlyStopping callback as part of\n",
    "                the callbacks argument, in which case the EarlyStopping callback you\n",
    "                specified will determine early stopping).\n",
    "            callbacks: List of Keras callbacks to apply during training and\n",
    "                validation.\n",
    "            validation_split: Float between 0 and 1. Defaults to 0.2.\n",
    "                Fraction of the training data to be used as validation data.\n",
    "                The model will set apart this fraction of the training data,\n",
    "                will not train on it, and will evaluate\n",
    "                the loss and any model metrics\n",
    "                on this data at the end of each epoch.\n",
    "                The validation data is selected from the last samples\n",
    "                in the `x` and `y` data provided, before shuffling. This argument is\n",
    "                not supported when `x` is a dataset.\n",
    "                The best model found would be fit on the entire dataset including the\n",
    "                validation data.\n",
    "            validation_data: Data on which to evaluate the loss and any model metrics\n",
    "                at the end of each epoch. The model will not be trained on this data.\n",
    "                `validation_data` will override `validation_split`. The type of the\n",
    "                validation data should be the same as the training data.\n",
    "                The best model found would be fit on the training dataset without the\n",
    "                validation data.\n",
    "            verbose: 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar,\n",
    "                2 = one line per epoch. Note that the progress bar is not\n",
    "                particularly useful when logged to a file, so verbose=2 is\n",
    "                recommended when not running interactively (eg, in a production\n",
    "                environment). Controls the verbosity of both KerasTuner search and\n",
    "                [keras.Model.fit](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit)\n",
    "            **kwargs: Any arguments supported by\n",
    "                [keras.Model.fit](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit).\n",
    "\n",
    "        # Returns\n",
    "            history: A Keras History object corresponding to the best model.\n",
    "                Its History.history attribute is a record of training\n",
    "                loss values and metrics values at successive epochs, as well as\n",
    "                validation loss values and validation metrics values (if applicable).\n",
    "        \"\"\"\n",
    "        # Check validation information.\n",
    "        if not validation_data and not validation_split:\n",
    "            raise ValueError(\n",
    "                \"Either validation_data or a non-zero validation_split \"\n",
    "                \"should be provided.\"\n",
    "            )\n",
    "\n",
    "        if validation_data:\n",
    "            validation_split = 0\n",
    "\n",
    "        dataset, validation_data = self._convert_to_dataset(\n",
    "            x=x, y=y, validation_data=validation_data, batch_size=batch_size\n",
    "        )\n",
    "        self._analyze_data(dataset)\n",
    "        self._build_hyper_pipeline(dataset)\n",
    "\n",
    "        # Split the data with validation_split.\n",
    "        if validation_data is None and validation_split:\n",
    "            dataset, validation_data = data_utils.split_dataset(\n",
    "                dataset, validation_split\n",
    "            )\n",
    "\n",
    "        history = self.tuner.search(\n",
    "            x=dataset,\n",
    "            epochs=epochs,\n",
    "            callbacks=callbacks,\n",
    "            validation_data=validation_data,\n",
    "            validation_split=validation_split,\n",
    "            verbose=verbose,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        return history\n",
    "\n",
    "    def _adapt(self, dataset, hms, batch_size):\n",
    "        if isinstance(dataset, tf.data.Dataset):\n",
    "            sources = data_utils.unzip_dataset(dataset)\n",
    "        else:\n",
    "            sources = nest.flatten(dataset)\n",
    "        adapted = []\n",
    "        for source, hm in zip(sources, hms):\n",
    "            source = hm.get_adapter().adapt(source, batch_size)\n",
    "            adapted.append(source)\n",
    "        if len(adapted) == 1:\n",
    "            return adapted[0]\n",
    "        return tf.data.Dataset.zip(tuple(adapted))\n",
    "\n",
    "    def _check_data_format(self, dataset, validation=False, predict=False):\n",
    "        \"\"\"Check if the dataset has the same number of IOs with the model.\"\"\"\n",
    "        if validation:\n",
    "            in_val = \" in validation_data\"\n",
    "            if isinstance(dataset, tf.data.Dataset):\n",
    "                x = dataset\n",
    "                y = None\n",
    "            else:\n",
    "                x, y = dataset\n",
    "        else:\n",
    "            in_val = \"\"\n",
    "            x, y = dataset\n",
    "\n",
    "        if isinstance(x, tf.data.Dataset) and y is not None:\n",
    "            raise ValueError(\n",
    "                \"Expected y to be None when x is \"\n",
    "                \"tf.data.Dataset{in_val}.\".format(in_val=in_val)\n",
    "            )\n",
    "\n",
    "        if isinstance(x, tf.data.Dataset):\n",
    "            if not predict:\n",
    "                x_shapes, y_shapes = data_utils.dataset_shape(x)\n",
    "                x_shapes = nest.flatten(x_shapes)\n",
    "                y_shapes = nest.flatten(y_shapes)\n",
    "            else:\n",
    "                x_shapes = nest.flatten(data_utils.dataset_shape(x))\n",
    "        else:\n",
    "            x_shapes = [a.shape for a in nest.flatten(x)]\n",
    "            if not predict:\n",
    "                y_shapes = [a.shape for a in nest.flatten(y)]\n",
    "\n",
    "        if len(x_shapes) != len(self.inputs):\n",
    "            raise ValueError(\n",
    "                \"Expected x{in_val} to have {input_num} arrays, \"\n",
    "                \"but got {data_num}\".format(\n",
    "                    in_val=in_val, input_num=len(self.inputs), data_num=len(x_shapes)\n",
    "                )\n",
    "            )\n",
    "        if not predict and len(y_shapes) != len(self.outputs):\n",
    "            raise ValueError(\n",
    "                \"Expected y{in_val} to have {output_num} arrays, \"\n",
    "                \"but got {data_num}\".format(\n",
    "                    in_val=in_val,\n",
    "                    output_num=len(self.outputs),\n",
    "                    data_num=len(y_shapes),\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def _analyze_data(self, dataset):\n",
    "        input_analysers = [node.get_analyser() for node in self.inputs]\n",
    "        output_analysers = [head.get_analyser() for head in self._heads]\n",
    "        analysers = input_analysers + output_analysers\n",
    "        for x, y in dataset:\n",
    "            x = nest.flatten(x)\n",
    "            y = nest.flatten(y)\n",
    "            for item, analyser in zip(x + y, analysers):\n",
    "                analyser.update(item)\n",
    "\n",
    "        for analyser in analysers:\n",
    "            analyser.finalize()\n",
    "\n",
    "        for hm, analyser in zip(self.inputs + self._heads, analysers):\n",
    "            hm.config_from_analyser(analyser)\n",
    "\n",
    "    def _build_hyper_pipeline(self, dataset):\n",
    "        self.tuner.hyper_pipeline = pipeline.HyperPipeline(\n",
    "            inputs=[node.get_hyper_preprocessors() for node in self.inputs],\n",
    "            outputs=[head.get_hyper_preprocessors() for head in self._heads],\n",
    "        )\n",
    "        self.tuner.hypermodel.hyper_pipeline = self.tuner.hyper_pipeline\n",
    "\n",
    "    def _convert_to_dataset(self, x, y, validation_data, batch_size):\n",
    "        \"\"\"Convert the data to tf.data.Dataset.\"\"\"\n",
    "        # TODO: Handle other types of input, zip dataset, tensor, dict.\n",
    "\n",
    "        # Convert training data.\n",
    "        self._check_data_format((x, y))\n",
    "        if isinstance(x, tf.data.Dataset):\n",
    "            dataset = x\n",
    "            x = dataset.map(lambda x, y: x)\n",
    "            y = dataset.map(lambda x, y: y)\n",
    "        x = self._adapt(x, self.inputs, batch_size)\n",
    "        y = self._adapt(y, self._heads, batch_size)\n",
    "        dataset = tf.data.Dataset.zip((x, y))\n",
    "\n",
    "        # Convert validation data\n",
    "        if validation_data:\n",
    "            self._check_data_format(validation_data, validation=True)\n",
    "            if isinstance(validation_data, tf.data.Dataset):\n",
    "                x = validation_data.map(lambda x, y: x)\n",
    "                y = validation_data.map(lambda x, y: y)\n",
    "            else:\n",
    "                x, y = validation_data\n",
    "            x = self._adapt(x, self.inputs, batch_size)\n",
    "            y = self._adapt(y, self._heads, batch_size)\n",
    "            validation_data = tf.data.Dataset.zip((x, y))\n",
    "\n",
    "        return dataset, validation_data\n",
    "\n",
    "    def _has_y(self, dataset):\n",
    "        \"\"\"Remove y from the tf.data.Dataset if exists.\"\"\"\n",
    "        shapes = data_utils.dataset_shape(dataset)\n",
    "        # Only one or less element in the first level.\n",
    "        if len(shapes) <= 1:\n",
    "            return False\n",
    "        # The first level has more than 1 element.\n",
    "        # The nest has 2 levels.\n",
    "        for shape in shapes:\n",
    "            if isinstance(shape, tuple):\n",
    "                return True\n",
    "        # The nest has one level.\n",
    "        # It matches the single IO case.\n",
    "        return len(shapes) == 2 and len(self.inputs) == 1 and len(self.outputs) == 1\n",
    "\n",
    "    def predict(self, x, batch_size=32, verbose=1, **kwargs):\n",
    "        \"\"\"Predict the output for a given testing data.\n",
    "\n",
    "        # Arguments\n",
    "            x: Any allowed types according to the input node. Testing data.\n",
    "            batch_size: Number of samples per batch.\n",
    "                If unspecified, batch_size will default to 32.\n",
    "            verbose: Verbosity mode. 0 = silent, 1 = progress bar.\n",
    "                Controls the verbosity of\n",
    "                [keras.Model.predict](https://tensorflow.org/api_docs/python/tf/keras/Model#predict)\n",
    "            **kwargs: Any arguments supported by keras.Model.predict.\n",
    "\n",
    "        # Returns\n",
    "            A list of numpy.ndarray objects or a single numpy.ndarray.\n",
    "            The predicted results.\n",
    "        \"\"\"\n",
    "        if isinstance(x, tf.data.Dataset) and self._has_y(x):\n",
    "            x = x.map(lambda x, y: x)\n",
    "        self._check_data_format((x, None), predict=True)\n",
    "        dataset = self._adapt(x, self.inputs, batch_size)\n",
    "        pipeline = self.tuner.get_best_pipeline()\n",
    "        model = self.tuner.get_best_model()\n",
    "        dataset = pipeline.transform_x(dataset)\n",
    "        dataset = tf.data.Dataset.zip((dataset, dataset))\n",
    "        y = model.predict(dataset, **kwargs)\n",
    "        y = utils.predict_with_adaptive_batch_size(\n",
    "            model=model, batch_size=batch_size, x=dataset, verbose=verbose, **kwargs\n",
    "        )\n",
    "        return pipeline.postprocess(y)\n",
    "\n",
    "    def evaluate(self, x, y=None, batch_size=32, verbose=1, **kwargs):\n",
    "        \"\"\"Evaluate the best model for the given data.\n",
    "\n",
    "        # Arguments\n",
    "            x: Any allowed types according to the input node. Testing data.\n",
    "            y: Any allowed types according to the head. Testing targets.\n",
    "                Defaults to None.\n",
    "            batch_size: Number of samples per batch.\n",
    "                If unspecified, batch_size will default to 32.\n",
    "            verbose: Verbosity mode. 0 = silent, 1 = progress bar.\n",
    "                Controls the verbosity of\n",
    "                [keras.Model.evaluate](http://tensorflow.org/api_docs/python/tf/keras/Model#evaluate)\n",
    "            **kwargs: Any arguments supported by keras.Model.evaluate.\n",
    "\n",
    "        # Returns\n",
    "            Scalar test loss (if the model has a single output and no metrics) or\n",
    "            list of scalars (if the model has multiple outputs and/or metrics).\n",
    "            The attribute model.metrics_names will give you the display labels for\n",
    "            the scalar outputs.\n",
    "        \"\"\"\n",
    "        self._check_data_format((x, y))\n",
    "        if isinstance(x, tf.data.Dataset):\n",
    "            dataset = x\n",
    "            x = dataset.map(lambda x, y: x)\n",
    "            y = dataset.map(lambda x, y: y)\n",
    "        x = self._adapt(x, self.inputs, batch_size)\n",
    "        y = self._adapt(y, self._heads, batch_size)\n",
    "        dataset = tf.data.Dataset.zip((x, y))\n",
    "        pipeline = self.tuner.get_best_pipeline()\n",
    "        dataset = pipeline.transform(dataset)\n",
    "        model = self.tuner.get_best_model()\n",
    "        return utils.evaluate_with_adaptive_batch_size(\n",
    "            model=model, batch_size=batch_size, x=dataset, verbose=verbose, **kwargs\n",
    "        )\n",
    "\n",
    "    def export_model(self):\n",
    "        \"\"\"Export the best Keras Model.\n",
    "\n",
    "        # Returns\n",
    "            tf.keras.Model instance. The best model found during the search, loaded\n",
    "            with trained weights.\n",
    "        \"\"\"\n",
    "        return self.tuner.get_best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f3fa9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-30 22:42:38 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-10-30 22:42:39, Running 3 Workers\n"
     ]
    }
   ],
   "source": [
    "from cerebro.backend import SparkBackend\n",
    "from cerebro.storage import LocalStore\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "input_node = ak.ImageInput()\n",
    "output_node = ak.Normalization()(input_node)\n",
    "output_node1 = ak.ConvBlock()(output_node)\n",
    "output_node2 = ak.ResNetBlock(version=\"v2\")(output_node)\n",
    "output_node = ak.Merge()([output_node1, output_node2])\n",
    "output_node = ak.ClassificationHead()(output_node)\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "   .master(\"local\") \\\n",
    "   .appName(\"test\") \\\n",
    "   .config(\"spark.executor.memory\", \"1gb\") \\\n",
    "   .getOrCreate()\n",
    "backend = SparkBackend(spark_context=spark.sparkContext, num_workers=3)\n",
    "store = LocalStore(prefix_path='/Users/zijian/Desktop/ucsd/cse234/project/playaround')\n",
    "\n",
    "auto_model = HyperHyperModel(\n",
    "    inputs=input_node, outputs=output_node, overwrite=True, max_trials=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afa8f86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_model.tuner_bind(backend,store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbd701f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
