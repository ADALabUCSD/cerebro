{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e29aafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/01 06:18:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-12-01 06:18:52, Running 1 Workers\n"
     ]
    }
   ],
   "source": [
    "from cerebro.backend import SparkBackend\n",
    "from cerebro.keras import SparkEstimator\n",
    "\n",
    "# datas storage for intermediate data and model artifacts.\n",
    "from cerebro.storage import LocalStore, HDFSStore\n",
    "\n",
    "# Model selection/AutoML methods.\n",
    "from cerebro.tune import GridSearch, RandomSearch, TPESearch\n",
    "\n",
    "# Utility functions for specifying the search space.\n",
    "from cerebro.tune import hp_choice, hp_uniform, hp_quniform, hp_loguniform, hp_qloguniform\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Cerebro Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "...\n",
    "work_dir = '/Users/zijian/Desktop/ucsd/cse234/project/cerebro-system/'\n",
    "backend = SparkBackend(spark_context=spark.sparkContext, num_workers=1)\n",
    "store = LocalStore(prefix_path=work_dir + 'test/')\n",
    "\n",
    "df = spark.read.format(\"libsvm\") \\\n",
    "    .option(\"numFeatures\", \"784\") \\\n",
    "    .load(\"/Users/zijian/Desktop/ucsd/cse234/project/mnist/mnist.scale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cecc1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "\n",
    "encoder = OneHotEncoderEstimator(dropLast=False)\n",
    "encoder.setInputCols([\"label\"])\n",
    "encoder.setOutputCols([\"label_OHE\"])\n",
    "\n",
    "encoder_model = encoder.fit(df)\n",
    "encoded = encoder_model.transform(df)\n",
    "\n",
    "feature_columns=['features']\n",
    "label_columns=['label_OHE']\n",
    "train_df, test_df = encoded.randomSplit([0.8, 0.2], seed=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ca16981",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_tuner.engine import hyperparameters\n",
    "import autokeras as ak\n",
    "from cerebro.nas.hphpmodel import HyperHyperModel\n",
    "\n",
    "img_shape = (28, 28, 1)\n",
    "\n",
    "input_node = ak.ImageInput()\n",
    "output_node = ak.ConvBlock(\n",
    "    kernel_size=hyperparameters.Fixed('kernel_size', value=3),\n",
    "    num_blocks=hyperparameters.Fixed('num_blocks', value=1),\n",
    "    num_layers=hyperparameters.Fixed('num_layers', value=2),\n",
    ")(input_node)\n",
    "output_node = ak.ClassificationHead()(output_node)\n",
    "am = HyperHyperModel(input_node, output_node, seed=2000)\n",
    "\n",
    "am.resource_bind(\n",
    "    backend=backend, \n",
    "    store=store,\n",
    "    feature_columns=feature_columns,\n",
    "    label_columns=label_columns,\n",
    "    evaluation_metric='accuracy', \n",
    ")\n",
    "\n",
    "am.tuner_bind(\n",
    "    tuner=\"randomsearch\", \n",
    "    hyperparameters=None, \n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=20,\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47ac1fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-12-01 06:18:56, Preparing Data\n",
      "CEREBRO => Time: 2021-12-01 06:18:56, Num Partitions: 12\n",
      "CEREBRO => Time: 2021-12-01 06:18:56, Writing DataFrames\n",
      "CEREBRO => Time: 2021-12-01 06:18:56, Train Data Path: file:///Users/zijian/Desktop/ucsd/cse234/project/cerebro-system/test/intermediate_train_data\n",
      "CEREBRO => Time: 2021-12-01 06:18:56, Val Data Path: file:///Users/zijian/Desktop/ucsd/cse234/project/cerebro-system/test/intermediate_val_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-12-01 06:19:04, Train Partitions: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-12-01 06:19:19, Val Partitions: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2021-12-01 06:19:37, Train Rows: 38401\n",
      "CEREBRO => Time: 2021-12-01 06:19:37, Val Rows: 9634\n",
      "CEREBRO => Time: 2021-12-01 06:19:37, Initializing Workers\n",
      "CEREBRO => Time: 2021-12-01 06:19:37, Initializing Data Loaders\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 9:>                                                          (0 + 1) / 1]"
     ]
    }
   ],
   "source": [
    "am.sys_setup(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15c8a554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check parquet dataset\n",
    "from cerebro.backend.spark import util\n",
    "\n",
    "ms = am.model_selection\n",
    "store = ms.store\n",
    "dataset_idx = None\n",
    "label_columns = ms.label_cols\n",
    "feature_columns = ms.feature_cols\n",
    "train_rows, val_rows, metadata, avg_row_size = \\\n",
    "        util.get_simple_meta_from_parquet(store,\n",
    "                                          schema_cols=label_columns + feature_columns,\n",
    "                                          sample_weight_col=None,\n",
    "                                          dataset_idx=dataset_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "005e0784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label_OHE': {'spark_data_type': pyspark.sql.types.BinaryType,\n",
       "  'is_sparse_vector_only': False,\n",
       "  'shape': None,\n",
       "  'intermediate_format': 'nochange',\n",
       "  'max_size': None},\n",
       " 'features': {'spark_data_type': pyspark.sql.types.BinaryType,\n",
       "  'is_sparse_vector_only': False,\n",
       "  'shape': None,\n",
       "  'intermediate_format': 'nochange',\n",
       "  'max_size': None}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f49b516d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataType(binary)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_path = store.get_train_data_path(dataset_idx)\n",
    "train_data = store.get_parquet_dataset(train_data_path)\n",
    "schema = train_data.schema.to_arrow_schema()\n",
    "schema.field('features').type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f21434bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-01 06:19:55.571168: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-12-01 06:19:55.571582: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[Stage 9:>                                                          (0 + 1) / 1]"
     ]
    }
   ],
   "source": [
    "# Create trial to generate estimator\n",
    "x = np.array(train_df.select(feature_columns).head(100))\n",
    "y = np.array(train_df.select(label_columns).head(100))\n",
    "x = [x[:,i] for i in range(x.shape[1])]\n",
    "x = [r.reshape((-1, *img_shape)) for r in x]\n",
    "y = np.squeeze(y,1)\n",
    "if len(y.shape) > 2:\n",
    "    raise ValueError(\n",
    "        \"We do not support multiple labels. Expect the target data for {name} to have shape \"\n",
    "        \"(batch_size, num_classes), \"\n",
    "        \"but got {shape}.\".format(name=self.name, shape=self.shape)\n",
    "    )\n",
    "dataset, validation_data = am._convert_to_dataset(\n",
    "    x=x, y=y, validation_data=None, batch_size=32\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "Analyze data analyse input and output data and config model inputs and heads\n",
    "\"\"\"\n",
    "am._analyze_data(dataset)\n",
    "\n",
    "\"\"\"\n",
    "Build preprocessing pipeline with tunable parameters\n",
    "\n",
    "Since the model is trained from workers which reads data from pre-distributed permanent storage, we will not consider tuning preprocessing currently.\n",
    "\"\"\"\n",
    "# self._build_hyper_pipeline(dataset)\n",
    "am.tuner.hyper_pipeline = None\n",
    "am.tuner.hypermodel.hyper_pipeline = None\n",
    "\n",
    "# Initial space\n",
    "tuner = am.tuner\n",
    "tuner.hypermodel.hypermodel.set_fit_args(0.2, epochs=10)\n",
    "\n",
    "# Populate initial search space.\n",
    "hp = tuner.oracle.get_space()\n",
    "tuner._prepare_model_IO(hp, dataset=dataset)\n",
    "tuner.hypermodel.build(hp)\n",
    "tuner.oracle.update_space(hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e912648",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = []\n",
    "trials = tuner.oracle.create_trials(1, tuner.tuner_id)\n",
    "estimators = tuner.trials2estimators(trials, dataset)\n",
    "\n",
    "estimator = estimators[0]\n",
    "keras_utils = estimator._get_keras_utils()\n",
    "run_id = estimator.getRunId()\n",
    "input_shapes, output_shapes = estimator.get_model_shapes()\n",
    "output_names = estimator.getModel().output_names\n",
    "sample_weight_col = estimator.getSampleWeightCol()\n",
    "make_dataset = keras_utils.make_dataset_fn(\n",
    "        feature_columns, label_columns, sample_weight_col, metadata,\n",
    "        input_shapes, output_shapes, output_names, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "995bd38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "floatx = tf.keras.backend.floatx()\n",
    "tf.keras.backend.set_floatx(floatx)\n",
    "\n",
    "from cerebro.backend import constants\n",
    "def _data_readers_fn(remote_store, shard_count, schema_fields, avg_row_size, cache_size_limit, pool_type, num_readers):\n",
    "    def _data_readers(index):\n",
    "        from petastorm import make_reader\n",
    "\n",
    "        PETASTORM_HDFS_DRIVER = constants.PETASTORM_HDFS_DRIVER\n",
    "\n",
    "        train_reader = make_reader(remote_store.train_data_path, shuffle_row_groups=False, num_epochs=None,\n",
    "                                   cur_shard=index,\n",
    "                                   shard_count=shard_count,\n",
    "                                   hdfs_driver=PETASTORM_HDFS_DRIVER,\n",
    "                                   schema_fields=schema_fields,\n",
    "                                   reader_pool_type=pool_type, workers_count=num_readers,\n",
    "                                   cache_type='local-disk',\n",
    "                                   cache_size_limit=cache_size_limit,\n",
    "                                   cache_row_size_estimate=avg_row_size,\n",
    "                                   cache_extra_settings={'cleanup': True})\n",
    "\n",
    "        if remote_store.val_data_path != '' and remote_store.val_data_path is not None:\n",
    "            val_reader = make_reader(remote_store.val_data_path, shuffle_row_groups=False, num_epochs=None,\n",
    "                                     cur_shard=index,\n",
    "                                     shard_count=shard_count,\n",
    "                                     hdfs_driver=PETASTORM_HDFS_DRIVER,\n",
    "                                     schema_fields=schema_fields,\n",
    "                                     reader_pool_type=pool_type, workers_count=num_readers,\n",
    "                                     cache_type='local-disk',\n",
    "                                     cache_size_limit=cache_size_limit,\n",
    "                                     cache_row_size_estimate=avg_row_size,\n",
    "                                     cache_extra_settings={'cleanup': True})\n",
    "        else:\n",
    "            val_reader = None\n",
    "\n",
    "        return train_reader, val_reader\n",
    "\n",
    "    return _data_readers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6be7d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autokeras model\n",
    "\n",
    "trial = trials[0]\n",
    "tuner._prepare_model_IO(trial.hyperparameters, dataset=dataset)\n",
    "model = tuner.hypermodel.build(trial.hyperparameters)\n",
    "tuner.adapt(model, dataset)\n",
    "optimizer_real = tf.keras.optimizers.Adam(lr=0.001)\n",
    "loss = 'categorical_crossentropy'\n",
    "model.compile(optimizer=optimizer_real, loss=loss, metrics=['accuracy'])\n",
    "model.save('debug_cpkt_ak.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d781357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "cast_to_float32 (CastToFloat (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d (SeparableC (None, 26, 26, 256)       521       \n",
      "_________________________________________________________________\n",
      "separable_conv2d_1 (Separabl (None, 24, 24, 32)        10528     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                46090     \n",
      "_________________________________________________________________\n",
      "classification_head_1 (Softm (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 57,139\n",
      "Trainable params: 57,139\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 9:>                                                          (0 + 1) / 1]"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cde1dd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/zijian/.pyenv/versions/3.7.6/envs/nocerebro/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:2561: calling DatasetV2.from_generator (from tensorflow.python.data.ops.dataset_ops) with output_types is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use output_signature instead\n",
      "WARNING:tensorflow:From /Users/zijian/.pyenv/versions/3.7.6/envs/nocerebro/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:2561: calling DatasetV2.from_generator (from tensorflow.python.data.ops.dataset_ops) with output_shapes is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use output_signature instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 9:>                                                          (0 + 1) / 1]"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "backend = ms.backend\n",
    "remote_store = store.to_remote(backend.spark_job_group, None)\n",
    "shard_count = backend._num_workers()\n",
    "schema_fields = ms.feature_cols + ms.label_cols\n",
    "_, _, _, avg_row_size = util.get_simple_meta_from_parquet(store, schema_fields, None, dataset_idx)\n",
    "data_readers_fn = _data_readers_fn(remote_store, shard_count, schema_fields, avg_row_size,\n",
    "                                               backend.settings.disk_cache_size_bytes,\n",
    "                                               backend.settings.data_readers_pool_type, backend.settings.num_data_readers)\n",
    "train_reader, val_reader = data_readers_fn(0)           \n",
    "user_shuffle_buffer_size = estimator.getShufflingBufferSize()\n",
    "if not user_shuffle_buffer_size:\n",
    "    shuffle_buffer_size = 1024 * 3\n",
    "else:\n",
    "    shuffle_buffer_size = user_shuffle_buffer_size\n",
    "train_data = make_dataset(train_reader, shuffle_buffer_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a798b080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected a shuffled dataset but input dataset `x` is not shuffled. Please invoke `shuffle()` on input dataset.\n",
      "Train on 1201 steps\n",
      " 237/1201 [====>.........................] - ETA: 44s - batch: 118.0000 - size: 1.0000 - loss: 2.0814 - accuracy: 0.2744"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 9:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1201/1201 [==============================] - 64s 52ms/step - batch: 600.0000 - size: 1.0000 - loss: 0.8908 - accuracy: 0.7060 2s - batch: 580.5000 - size: 1.0000 -\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 1) / 1]"
     ]
    }
   ],
   "source": [
    "# training \n",
    "import io\n",
    "import h5py\n",
    "import math\n",
    "\n",
    "def _deserialize_keras_model_fn():\n",
    "    def deserialize_keras_model(model_bytes, load_model_fn):\n",
    "        \"\"\"Deserialize model from byte array encoded in base 64.\"\"\"\n",
    "        # model_bytes = codec.loads_base64(model_bytes)\n",
    "        bio = io.BytesIO(model_bytes)\n",
    "        with h5py.File(bio, 'r') as f:\n",
    "            return load_model_fn(f)\n",
    "\n",
    "    return deserialize_keras_model\n",
    "\n",
    "deserialize_keras_model = _deserialize_keras_model_fn()\n",
    "custom_objects = estimator.getCustomObjects()\n",
    "with tf.keras.utils.custom_object_scope(custom_objects):\n",
    "        model = deserialize_keras_model(\n",
    "        store.read('debug_cpkt_ak.h5'), lambda x: tf.keras.models.load_model(x))\n",
    "steps_per_epoch = int(math.ceil(train_rows / 32 / ms.backend._num_workers()))\n",
    "result = model.fit(train_data, epochs=1, steps_per_epoch=steps_per_epoch).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef75f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pure keras model training using parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d3fa4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=img_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(10, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.001)\n",
    "loss = 'categorical_crossentropy'\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "model.save('debug_cpkt_normal.h5')\n",
    "# fit_sub_epoch_fn = keras_utils.fit_sub_epoch_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79ae05fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected a shuffled dataset but input dataset `x` is not shuffled. Please invoke `shuffle()` on input dataset.\n",
      "Train on 1201 steps\n",
      "1181/1201 [============================>.] - ETA: 0s - batch: 590.0000 - size: 1.0000 - loss: 0.4725 - accuracy: 0.8476"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/dg/4jghvsmd1dl77gj1fqmzzf_80000gn/T/ipykernel_54053/2059395462.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m         store.read('debug_cpkt_normal.h5'), lambda x: tf.keras.models.load_model(x))\n\u001b[1;32m     21\u001b[0m \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_rows\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m32\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_workers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.7.6/envs/nocerebro/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_v1.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    806\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    809\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.6/envs/nocerebro/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays_v1.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.6/envs/nocerebro/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays_v1.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             \u001b[0mactual_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mis_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.6/envs/nocerebro/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3956\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3957\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3958\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3959\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.6/envs/nocerebro/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1480\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1481\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1482\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1483\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training \n",
    "import io\n",
    "import h5py\n",
    "import math\n",
    "\n",
    "def _deserialize_keras_model_fn():\n",
    "    def deserialize_keras_model(model_bytes, load_model_fn):\n",
    "        \"\"\"Deserialize model from byte array encoded in base 64.\"\"\"\n",
    "        # model_bytes = codec.loads_base64(model_bytes)\n",
    "        bio = io.BytesIO(model_bytes)\n",
    "        with h5py.File(bio, 'r') as f:\n",
    "            return load_model_fn(f)\n",
    "\n",
    "    return deserialize_keras_model\n",
    "\n",
    "deserialize_keras_model = _deserialize_keras_model_fn()\n",
    "custom_objects = estimator.getCustomObjects()\n",
    "with tf.keras.utils.custom_object_scope(custom_objects):\n",
    "        model = deserialize_keras_model(\n",
    "        store.read('debug_cpkt_normal.h5'), lambda x: tf.keras.models.load_model(x))\n",
    "steps_per_epoch = int(math.ceil(train_rows / 32 / ms.backend._num_workers()))\n",
    "result = model.fit(train_data, epochs=1, steps_per_epoch=steps_per_epoch).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "144602e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from petastorm.tf_utils import make_petastorm_dataset\n",
    "from petastorm import make_reader\n",
    "shard_count = backend._num_workers()\n",
    "PETASTORM_HDFS_DRIVER = constants.PETASTORM_HDFS_DRIVER\n",
    "pool_type = backend.settings.data_readers_pool_type\n",
    "num_readers = backend.settings.num_data_readers\n",
    "cache_size_limit = backend.settings.disk_cache_size_bytes\n",
    "train_reader = make_reader(remote_store.train_data_path, \n",
    "                           shuffle_row_groups=False, num_epochs=None,\n",
    "                           cur_shard=0,\n",
    "                           shard_count=shard_count,\n",
    "                           hdfs_driver=PETASTORM_HDFS_DRIVER,\n",
    "                           schema_fields=schema_fields,\n",
    "                           reader_pool_type=pool_type, workers_count=num_readers,\n",
    "                           cache_type='local-disk',\n",
    "                           cache_size_limit=cache_size_limit,\n",
    "                           cache_row_size_estimate=avg_row_size,\n",
    "                           cache_extra_settings={'cleanup': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6e807fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/zijian/.pyenv/versions/3.7.6/envs/nocerebro/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:2561: calling DatasetV2.from_generator (from tensorflow.python.data.ops.dataset_ops) with output_types is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use output_signature instead\n",
      "WARNING:tensorflow:From /Users/zijian/.pyenv/versions/3.7.6/envs/nocerebro/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:2561: calling DatasetV2.from_generator (from tensorflow.python.data.ops.dataset_ops) with output_shapes is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use output_signature instead\n"
     ]
    }
   ],
   "source": [
    "dataset = make_petastorm_dataset(train_reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2796af83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: petastorm_schema_view_view(features=(784,), label_OHE=(10,)), types: petastorm_schema_view_view(features=tf.float64, label_OHE=tf.float64)>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 1) / 1]"
     ]
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cf61cc9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: (((None, 28, 28, 1),), ((None, 10),)), types: ((tf.float64,), (tf.float64,))>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cerebro.keras.spark.util import _prep_data_fn\n",
    "has_sparse_col = any(metadata[col]['is_sparse_vector_only']\n",
    "                             for col in label_columns + feature_columns)\n",
    "\n",
    "prep_data_tf_keras = _prep_data_fn(\n",
    "            has_sparse_col, sample_weight_col, feature_columns,\n",
    "            label_columns, input_shapes, output_shapes, output_names)\n",
    "\n",
    "bdataset = dataset.batch(32) \\\n",
    "        .map(prep_data_tf_keras, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "bdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8fabd095",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "__iter__() is only supported inside of tf.function or when eager execution is enabled.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/dg/4jghvsmd1dl77gj1fqmzzf_80000gn/T/ipykernel_50003/339673468.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.6/envs/nocerebro/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2775\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2776\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.6/envs/nocerebro/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n\u001b[0m\u001b[1;32m    425\u001b[0m                          \"or when eager execution is enabled.\")\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: __iter__() is only supported inside of tf.function or when eager execution is enabled."
     ]
    }
   ],
   "source": [
    "for d in bdataset:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "30827d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=img_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(10, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.001)\n",
    "loss = 'categorical_crossentropy'\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2b588ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected a shuffled dataset but input dataset `x` is not shuffled. Please invoke `shuffle()` on input dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-01 05:17:03.443549: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:196] None of the MLIR optimization passes are enabled (registered 0 passes)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on None steps\n",
      "    668/Unknown - 21s 31ms/step - batch: 333.5000 - size: 1.0000 - loss: 0.8228 - accuracy: 0.7258"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 9:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1244/Unknown - 38s 30ms/step - batch: 621.5000 - size: 1.0000 - loss: 0.5248 - accuracy: 0.8285 36s 30ms/step - batch: 602.5000 - size: 1."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/dg/4jghvsmd1dl77gj1fqmzzf_80000gn/T/ipykernel_50003/659898727.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.7.6/envs/nocerebro/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_v1.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    806\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    809\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.6/envs/nocerebro/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays_v1.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.6/envs/nocerebro/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays_v1.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             \u001b[0mactual_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mis_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.6/envs/nocerebro/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3956\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3957\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3958\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3959\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.6/envs/nocerebro/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1480\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1481\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1482\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1483\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(bdataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5af698",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
